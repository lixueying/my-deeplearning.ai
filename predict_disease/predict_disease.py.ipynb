{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from __future__ import print_function\n",
    "import collections\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "from matplotlib import pylab\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "from sklearn.manifold import TSNE\n",
    "from tf_utils import load_dataset, random_mini_batches, convert_to_one_hot, predict\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     sbp  tobacco    ldl  adiposity  famhist  typea  obesity  alcohol  age  \\\n",
      "0    160    12.00   5.73      23.11  Present     49    25.30    97.20   52   \n",
      "1    144     0.01   4.41      28.61   Absent     55    28.87     2.06   63   \n",
      "2    118     0.08   3.48      32.28  Present     52    29.14     3.81   46   \n",
      "3    170     7.50   6.41      38.03  Present     51    31.99    24.26   58   \n",
      "4    134    13.60   3.50      27.78  Present     60    25.99    57.34   49   \n",
      "5    132     6.20   6.47      36.21  Present     62    30.77    14.14   45   \n",
      "6    142     4.05   3.38      16.20   Absent     59    20.81     2.62   38   \n",
      "7    114     4.08   4.59      14.60  Present     62    23.11     6.72   58   \n",
      "8    114     0.00   3.83      19.40  Present     49    24.86     2.49   29   \n",
      "9    132     0.00   5.80      30.96  Present     69    30.11     0.00   53   \n",
      "10   206     6.00   2.95      32.27   Absent     72    26.81    56.06   60   \n",
      "11   134    14.10   4.44      22.39  Present     65    23.09     0.00   40   \n",
      "12   118     0.00   1.88      10.05   Absent     59    21.57     0.00   17   \n",
      "13   132     0.00   1.87      17.21   Absent     49    23.63     0.97   15   \n",
      "14   112     9.65   2.29      17.20  Present     54    23.53     0.68   53   \n",
      "15   117     1.53   2.44      28.95  Present     35    25.89    30.03   46   \n",
      "16   120     7.50  15.33      22.00   Absent     60    25.31    34.49   49   \n",
      "17   146    10.50   8.29      35.36  Present     78    32.73    13.89   53   \n",
      "18   158     2.60   7.46      34.07  Present     61    29.30    53.28   62   \n",
      "19   124    14.00   6.23      35.96  Present     45    30.09     0.00   59   \n",
      "20   106     1.61   1.74      12.32   Absent     74    20.92    13.37   20   \n",
      "21   132     7.90   2.85      26.50  Present     51    26.16    25.71   44   \n",
      "22   150     0.30   6.38      33.99  Present     62    24.64     0.00   50   \n",
      "23   138     0.60   3.81      28.66   Absent     54    28.70     1.46   58   \n",
      "24   142    18.20   4.34      24.38   Absent     61    26.19     0.00   50   \n",
      "25   124     4.00  12.42      31.29  Present     54    23.23     2.06   42   \n",
      "26   118     6.00   9.65      33.91   Absent     60    38.80     0.00   48   \n",
      "27   145     9.10   5.24      27.55   Absent     59    20.96    21.60   61   \n",
      "28   144     4.09   5.55      31.40  Present     60    29.43     5.55   56   \n",
      "29   146     0.00   6.62      25.69   Absent     60    28.07     8.23   63   \n",
      "..   ...      ...    ...        ...      ...    ...      ...      ...  ...   \n",
      "432  136     0.00   4.00      19.06   Absent     40    21.94     2.06   16   \n",
      "433  120     0.00   2.46      13.39   Absent     47    22.01     0.51   18   \n",
      "434  132     0.00   3.55       8.66  Present     61    18.50     3.87   16   \n",
      "435  136     0.00   1.77      20.37   Absent     45    21.51     2.06   16   \n",
      "436  138     0.00   1.86      18.35  Present     59    25.38     6.51   17   \n",
      "437  138     0.06   4.15      20.66   Absent     49    22.59     2.49   16   \n",
      "438  130     1.22   3.30      13.65   Absent     50    21.40     3.81   31   \n",
      "439  130     4.00   2.40      17.42   Absent     60    22.05     0.00   40   \n",
      "440  110     0.00   7.14      28.28   Absent     57    29.00     0.00   32   \n",
      "441  120     0.00   3.98      13.19  Present     47    21.89     0.00   16   \n",
      "442  166     6.00   8.80      37.89   Absent     39    28.70    43.20   52   \n",
      "443  134     0.57   4.75      23.07   Absent     67    26.33     0.00   37   \n",
      "444  142     3.00   3.69      25.10   Absent     60    30.08    38.88   27   \n",
      "445  136     2.80   2.53       9.28  Present     61    20.70     4.55   25   \n",
      "446  142     0.00   4.32      25.22   Absent     47    28.92     6.53   34   \n",
      "447  130     0.00   1.88      12.51  Present     52    20.28     0.00   17   \n",
      "448  124     1.80   3.74      16.64  Present     42    22.26    10.49   20   \n",
      "449  144     4.00   5.03      25.78  Present     57    27.55    90.00   48   \n",
      "450  136     1.81   3.31       6.74   Absent     63    19.57    24.94   24   \n",
      "451  120     0.00   2.77      13.35   Absent     67    23.37     1.03   18   \n",
      "452  154     5.53   3.20      28.81  Present     61    26.15    42.79   42   \n",
      "453  124     1.60   7.22      39.68  Present     36    31.50     0.00   51   \n",
      "454  146     0.64   4.82      28.02   Absent     60    28.11     8.23   39   \n",
      "455  128     2.24   2.83      26.48   Absent     48    23.96    47.42   27   \n",
      "456  170     0.40   4.11      42.06  Present     56    33.10     2.06   57   \n",
      "457  214     0.40   5.98      31.72   Absent     64    28.45     0.00   58   \n",
      "458  182     4.20   4.41      32.10   Absent     52    28.61    18.72   52   \n",
      "459  108     3.00   1.59      15.23   Absent     40    20.09    26.64   55   \n",
      "460  118     5.40  11.61      30.79   Absent     64    27.35    23.97   40   \n",
      "461  132     0.00   4.82      33.41  Present     62    14.70     0.00   46   \n",
      "\n",
      "     chd  \n",
      "0      1  \n",
      "1      1  \n",
      "2      0  \n",
      "3      1  \n",
      "4      1  \n",
      "5      0  \n",
      "6      0  \n",
      "7      1  \n",
      "8      0  \n",
      "9      1  \n",
      "10     1  \n",
      "11     1  \n",
      "12     0  \n",
      "13     0  \n",
      "14     0  \n",
      "15     0  \n",
      "16     0  \n",
      "17     1  \n",
      "18     1  \n",
      "19     1  \n",
      "20     1  \n",
      "21     0  \n",
      "22     0  \n",
      "23     0  \n",
      "24     0  \n",
      "25     1  \n",
      "26     0  \n",
      "27     1  \n",
      "28     0  \n",
      "29     1  \n",
      "..   ...  \n",
      "432    0  \n",
      "433    0  \n",
      "434    0  \n",
      "435    0  \n",
      "436    0  \n",
      "437    0  \n",
      "438    0  \n",
      "439    0  \n",
      "440    0  \n",
      "441    0  \n",
      "442    0  \n",
      "443    0  \n",
      "444    0  \n",
      "445    0  \n",
      "446    1  \n",
      "447    0  \n",
      "448    0  \n",
      "449    1  \n",
      "450    0  \n",
      "451    0  \n",
      "452    0  \n",
      "453    1  \n",
      "454    1  \n",
      "455    1  \n",
      "456    0  \n",
      "457    0  \n",
      "458    1  \n",
      "459    0  \n",
      "460    0  \n",
      "461    1  \n",
      "\n",
      "[462 rows x 10 columns]\n",
      "Index(['sbp', 'tobacco', 'ldl', 'adiposity', 'famhist', 'typea', 'obesity',\n",
      "       'alcohol', 'age'],\n",
      "      dtype='object')\n",
      "     sbp  tobacco    ldl  adiposity  famhist  typea  obesity  alcohol  age\n",
      "0    160    12.00   5.73      23.11  Present     49    25.30    97.20   52\n",
      "1    144     0.01   4.41      28.61   Absent     55    28.87     2.06   63\n",
      "2    118     0.08   3.48      32.28  Present     52    29.14     3.81   46\n",
      "3    170     7.50   6.41      38.03  Present     51    31.99    24.26   58\n",
      "4    134    13.60   3.50      27.78  Present     60    25.99    57.34   49\n",
      "5    132     6.20   6.47      36.21  Present     62    30.77    14.14   45\n",
      "6    142     4.05   3.38      16.20   Absent     59    20.81     2.62   38\n",
      "7    114     4.08   4.59      14.60  Present     62    23.11     6.72   58\n",
      "8    114     0.00   3.83      19.40  Present     49    24.86     2.49   29\n",
      "9    132     0.00   5.80      30.96  Present     69    30.11     0.00   53\n",
      "10   206     6.00   2.95      32.27   Absent     72    26.81    56.06   60\n",
      "11   134    14.10   4.44      22.39  Present     65    23.09     0.00   40\n",
      "12   118     0.00   1.88      10.05   Absent     59    21.57     0.00   17\n",
      "13   132     0.00   1.87      17.21   Absent     49    23.63     0.97   15\n",
      "14   112     9.65   2.29      17.20  Present     54    23.53     0.68   53\n",
      "15   117     1.53   2.44      28.95  Present     35    25.89    30.03   46\n",
      "16   120     7.50  15.33      22.00   Absent     60    25.31    34.49   49\n",
      "17   146    10.50   8.29      35.36  Present     78    32.73    13.89   53\n",
      "18   158     2.60   7.46      34.07  Present     61    29.30    53.28   62\n",
      "19   124    14.00   6.23      35.96  Present     45    30.09     0.00   59\n",
      "20   106     1.61   1.74      12.32   Absent     74    20.92    13.37   20\n",
      "21   132     7.90   2.85      26.50  Present     51    26.16    25.71   44\n",
      "22   150     0.30   6.38      33.99  Present     62    24.64     0.00   50\n",
      "23   138     0.60   3.81      28.66   Absent     54    28.70     1.46   58\n",
      "24   142    18.20   4.34      24.38   Absent     61    26.19     0.00   50\n",
      "25   124     4.00  12.42      31.29  Present     54    23.23     2.06   42\n",
      "26   118     6.00   9.65      33.91   Absent     60    38.80     0.00   48\n",
      "27   145     9.10   5.24      27.55   Absent     59    20.96    21.60   61\n",
      "28   144     4.09   5.55      31.40  Present     60    29.43     5.55   56\n",
      "29   146     0.00   6.62      25.69   Absent     60    28.07     8.23   63\n",
      "..   ...      ...    ...        ...      ...    ...      ...      ...  ...\n",
      "432  136     0.00   4.00      19.06   Absent     40    21.94     2.06   16\n",
      "433  120     0.00   2.46      13.39   Absent     47    22.01     0.51   18\n",
      "434  132     0.00   3.55       8.66  Present     61    18.50     3.87   16\n",
      "435  136     0.00   1.77      20.37   Absent     45    21.51     2.06   16\n",
      "436  138     0.00   1.86      18.35  Present     59    25.38     6.51   17\n",
      "437  138     0.06   4.15      20.66   Absent     49    22.59     2.49   16\n",
      "438  130     1.22   3.30      13.65   Absent     50    21.40     3.81   31\n",
      "439  130     4.00   2.40      17.42   Absent     60    22.05     0.00   40\n",
      "440  110     0.00   7.14      28.28   Absent     57    29.00     0.00   32\n",
      "441  120     0.00   3.98      13.19  Present     47    21.89     0.00   16\n",
      "442  166     6.00   8.80      37.89   Absent     39    28.70    43.20   52\n",
      "443  134     0.57   4.75      23.07   Absent     67    26.33     0.00   37\n",
      "444  142     3.00   3.69      25.10   Absent     60    30.08    38.88   27\n",
      "445  136     2.80   2.53       9.28  Present     61    20.70     4.55   25\n",
      "446  142     0.00   4.32      25.22   Absent     47    28.92     6.53   34\n",
      "447  130     0.00   1.88      12.51  Present     52    20.28     0.00   17\n",
      "448  124     1.80   3.74      16.64  Present     42    22.26    10.49   20\n",
      "449  144     4.00   5.03      25.78  Present     57    27.55    90.00   48\n",
      "450  136     1.81   3.31       6.74   Absent     63    19.57    24.94   24\n",
      "451  120     0.00   2.77      13.35   Absent     67    23.37     1.03   18\n",
      "452  154     5.53   3.20      28.81  Present     61    26.15    42.79   42\n",
      "453  124     1.60   7.22      39.68  Present     36    31.50     0.00   51\n",
      "454  146     0.64   4.82      28.02   Absent     60    28.11     8.23   39\n",
      "455  128     2.24   2.83      26.48   Absent     48    23.96    47.42   27\n",
      "456  170     0.40   4.11      42.06  Present     56    33.10     2.06   57\n",
      "457  214     0.40   5.98      31.72   Absent     64    28.45     0.00   58\n",
      "458  182     4.20   4.41      32.10   Absent     52    28.61    18.72   52\n",
      "459  108     3.00   1.59      15.23   Absent     40    20.09    26.64   55\n",
      "460  118     5.40  11.61      30.79   Absent     64    27.35    23.97   40\n",
      "461  132     0.00   4.82      33.41  Present     62    14.70     0.00   46\n",
      "\n",
      "[462 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "# download data\n",
    "raw_data = pd.read_csv('heart.csv')\n",
    "\n",
    "print(raw_data)\n",
    "print(raw_data.columns[:9])\n",
    "print(raw_data.loc[:,raw_data.columns[:9]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sbp</th>\n",
       "      <th>tobacco</th>\n",
       "      <th>ldl</th>\n",
       "      <th>adiposity</th>\n",
       "      <th>famhist</th>\n",
       "      <th>typea</th>\n",
       "      <th>obesity</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>age</th>\n",
       "      <th>chd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4.620000e+02</td>\n",
       "      <td>4.620000e+02</td>\n",
       "      <td>4.620000e+02</td>\n",
       "      <td>4.620000e+02</td>\n",
       "      <td>4.620000e+02</td>\n",
       "      <td>4.620000e+02</td>\n",
       "      <td>4.620000e+02</td>\n",
       "      <td>4.620000e+02</td>\n",
       "      <td>4.620000e+02</td>\n",
       "      <td>462.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-2.691450e-17</td>\n",
       "      <td>-4.205390e-18</td>\n",
       "      <td>4.998407e-17</td>\n",
       "      <td>4.277483e-17</td>\n",
       "      <td>4.998407e-17</td>\n",
       "      <td>-1.826341e-17</td>\n",
       "      <td>8.026288e-17</td>\n",
       "      <td>3.172066e-17</td>\n",
       "      <td>9.612321e-19</td>\n",
       "      <td>0.346320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.751822e-01</td>\n",
       "      <td>1.472123e-01</td>\n",
       "      <td>1.443142e-01</td>\n",
       "      <td>2.176419e-01</td>\n",
       "      <td>4.933567e-01</td>\n",
       "      <td>1.510390e-01</td>\n",
       "      <td>1.321732e-01</td>\n",
       "      <td>1.663228e-01</td>\n",
       "      <td>2.981420e-01</td>\n",
       "      <td>0.476313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-3.190328e-01</td>\n",
       "      <td>-1.165272e-01</td>\n",
       "      <td>-2.620435e-01</td>\n",
       "      <td>-5.221463e-01</td>\n",
       "      <td>-5.844156e-01</td>\n",
       "      <td>-6.169830e-01</td>\n",
       "      <td>-3.558379e-01</td>\n",
       "      <td>-1.157986e-01</td>\n",
       "      <td>-5.676738e-01</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-1.224516e-01</td>\n",
       "      <td>-1.148445e-01</td>\n",
       "      <td>-1.015906e-01</td>\n",
       "      <td>-1.575310e-01</td>\n",
       "      <td>-5.844156e-01</td>\n",
       "      <td>-9.390609e-02</td>\n",
       "      <td>-9.595711e-02</td>\n",
       "      <td>-1.123337e-01</td>\n",
       "      <td>-2.411432e-01</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-3.698154e-02</td>\n",
       "      <td>-5.242466e-02</td>\n",
       "      <td>-2.789719e-02</td>\n",
       "      <td>1.981170e-02</td>\n",
       "      <td>4.155844e-01</td>\n",
       "      <td>-1.598402e-03</td>\n",
       "      <td>-7.500394e-03</td>\n",
       "      <td>-6.477610e-02</td>\n",
       "      <td>4.457108e-02</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>8.267658e-02</td>\n",
       "      <td>5.975483e-02</td>\n",
       "      <td>7.314811e-02</td>\n",
       "      <td>1.628187e-01</td>\n",
       "      <td>4.155844e-01</td>\n",
       "      <td>1.060939e-01</td>\n",
       "      <td>7.695695e-02</td>\n",
       "      <td>4.652562e-02</td>\n",
       "      <td>2.486527e-01</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>6.809672e-01</td>\n",
       "      <td>8.834728e-01</td>\n",
       "      <td>7.379565e-01</td>\n",
       "      <td>4.778537e-01</td>\n",
       "      <td>4.155844e-01</td>\n",
       "      <td>3.830170e-01</td>\n",
       "      <td>6.441621e-01</td>\n",
       "      <td>8.842014e-01</td>\n",
       "      <td>4.323262e-01</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                sbp       tobacco           ldl     adiposity       famhist  \\\n",
       "count  4.620000e+02  4.620000e+02  4.620000e+02  4.620000e+02  4.620000e+02   \n",
       "mean  -2.691450e-17 -4.205390e-18  4.998407e-17  4.277483e-17  4.998407e-17   \n",
       "std    1.751822e-01  1.472123e-01  1.443142e-01  2.176419e-01  4.933567e-01   \n",
       "min   -3.190328e-01 -1.165272e-01 -2.620435e-01 -5.221463e-01 -5.844156e-01   \n",
       "25%   -1.224516e-01 -1.148445e-01 -1.015906e-01 -1.575310e-01 -5.844156e-01   \n",
       "50%   -3.698154e-02 -5.242466e-02 -2.789719e-02  1.981170e-02  4.155844e-01   \n",
       "75%    8.267658e-02  5.975483e-02  7.314811e-02  1.628187e-01  4.155844e-01   \n",
       "max    6.809672e-01  8.834728e-01  7.379565e-01  4.778537e-01  4.155844e-01   \n",
       "\n",
       "              typea       obesity       alcohol           age         chd  \n",
       "count  4.620000e+02  4.620000e+02  4.620000e+02  4.620000e+02  462.000000  \n",
       "mean  -1.826341e-17  8.026288e-17  3.172066e-17  9.612321e-19    0.346320  \n",
       "std    1.510390e-01  1.321732e-01  1.663228e-01  2.981420e-01    0.476313  \n",
       "min   -6.169830e-01 -3.558379e-01 -1.157986e-01 -5.676738e-01    0.000000  \n",
       "25%   -9.390609e-02 -9.595711e-02 -1.123337e-01 -2.411432e-01    0.000000  \n",
       "50%   -1.598402e-03 -7.500394e-03 -6.477610e-02  4.457108e-02    0.000000  \n",
       "75%    1.060939e-01  7.695695e-02  4.652562e-02  2.486527e-01    1.000000  \n",
       "max    3.830170e-01  6.441621e-01  8.842014e-01  4.323262e-01    1.000000  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label = raw_data['chd']\n",
    "# data = raw_data.loc[:,raw_data.columns[:9]].replace(['Present', 'Absent'],[0, 1])\n",
    "# print(data)\n",
    "# data = data.apply(lambda x: (x - np.mean(x)) / (np.max(x) - np.min(x)), axis=0)\n",
    "# all_data = data\n",
    "# all_data['chd'] = label\n",
    "# all_data.describe()\n",
    "# print(all_data)\n",
    "\n",
    "data = raw_data.loc[:,raw_data.columns[:9]].replace(['Present', 'Absent'],[0, 1])\n",
    "# 归一化data\n",
    "data = data.apply(lambda x: (x - np.mean(x)) / (np.max(x) - np.min(x)), axis=0)\n",
    "\n",
    "all_data = data\n",
    "all_data['chd'] = label\n",
    "all_data.describe()\n",
    "\n",
    "# print(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          sbp   tobacco       ldl  adiposity   famhist     typea   obesity  \\\n",
      "27   0.057036  0.175139  0.034821   0.059952  0.415584  0.090709 -0.159477   \n",
      "11  -0.036982  0.335396 -0.020929  -0.084384 -0.584416  0.183017 -0.092664   \n",
      "415 -0.036982 -0.081271 -0.083646  -0.139769 -0.584416  0.075325 -0.047180   \n",
      "382  0.014301 -0.062681  0.464786   0.115616 -0.584416  0.321479  0.147613   \n",
      "106 -0.259204 -0.068450 -0.028594  -0.011657  0.415584  0.198402 -0.117758   \n",
      "215  0.185241  0.132511  0.232033   0.262749  0.415584  0.167632  0.160160   \n",
      "228 -0.002794 -0.052425  0.025761   0.167644 -0.584416 -0.063137  0.037826   \n",
      "18   0.168147 -0.033194  0.189524   0.242329 -0.584416  0.121479  0.102129   \n",
      "17   0.065583  0.220011  0.247364   0.278413 -0.584416  0.383017  0.209720   \n",
      "390 -0.071170 -0.099220 -0.077375  -0.094454 -0.584416  0.244555 -0.053454   \n",
      "113  0.304899 -0.116527  0.259211   0.271140 -0.584416 -0.278521 -0.024282   \n",
      "32  -0.139546  0.095011  0.058514   0.294917 -0.584416 -0.001598  0.063547   \n",
      "221  0.219429 -0.100502  0.153984   0.398133 -0.584416 -0.093906  0.492970   \n",
      "123 -0.002794  0.027704 -0.131730   0.131560  0.415584  0.029171 -0.039652   \n",
      "333  0.116865  0.506870 -0.036260   0.123168  0.415584 -0.386214 -0.065687   \n",
      "166 -0.242110  0.273216  0.017399   0.088203  0.415584 -0.140060  0.034375   \n",
      "269 -0.019888  0.101421  0.216005   0.149182 -0.584416  0.075325  0.004890   \n",
      "4   -0.036982  0.319370 -0.086434   0.066385 -0.584416  0.106094 -0.001697   \n",
      "310  0.304899 -0.004348  0.036214  -0.096132 -0.584416 -0.263137 -0.125600   \n",
      "346  0.133959  0.027704  0.000674  -0.052776 -0.584416 -0.155445 -0.008912   \n",
      "39  -0.019888  0.242447  0.074542   0.180231 -0.584416  0.336863  0.051314   \n",
      "211 -0.139546 -0.084476  0.079420   0.263028 -0.584416  0.244555  0.163924   \n",
      "29   0.065583 -0.116527  0.130988   0.007924  0.415584  0.106094  0.063547   \n",
      "387  0.082677  0.027704  0.400674   0.219952  0.415584 -0.047752 -0.003893   \n",
      "303 -0.088264 -0.116527  0.255727   0.095476 -0.584416  0.106094  0.025592   \n",
      "31   0.168147 -0.083835  0.110779  -0.042706  0.415584  0.198402 -0.122776   \n",
      "183  0.151053  0.011678 -0.187479  -0.165783 -0.584416 -0.047752 -0.143165   \n",
      "318  0.253617  0.248857  0.023671   0.035056 -0.584416  0.044555  0.031239   \n",
      "345  0.510027 -0.099861  0.498235   0.063588 -0.584416 -0.078521  0.073899   \n",
      "270 -0.071170 -0.116527 -0.040441   0.392259 -0.584416 -0.109291  0.124400   \n",
      "..        ...       ...       ...        ...       ...       ...       ...   \n",
      "70  -0.173734 -0.116527 -0.074587  -0.371377  0.415584 -0.032368 -0.216252   \n",
      "272 -0.019888 -0.072937 -0.110127  -0.291937 -0.584416  0.044555 -0.033379   \n",
      "278 -0.207922 -0.116527 -0.147061  -0.439629  0.415584 -0.124675 -0.255775   \n",
      "372 -0.088264 -0.065245  0.046667   0.108903  0.415584  0.229171  0.104639   \n",
      "279 -0.173734 -0.112681 -0.103159  -0.367741  0.415584  0.029171 -0.106465   \n",
      "250  0.168147  0.081229  0.235517   0.149462  0.415584 -0.109291  0.056333   \n",
      "366  0.099771 -0.116527  0.017399   0.064987  0.415584  0.059940  0.152945   \n",
      "283 -0.036982 -0.020373 -0.109430  -0.209699  0.415584 -0.278521  0.010222   \n",
      "210 -0.036982 -0.114925  0.229246   0.071140  0.415584 -0.078521  0.026220   \n",
      "197  0.202335 -0.022937 -0.077375   0.165686  0.415584  0.136863  0.173961   \n",
      "331 -0.105358 -0.017168 -0.190267   0.211560 -0.584416  0.044555  0.081113   \n",
      "79  -0.259204 -0.103707  0.081510  -0.069559 -0.584416  0.059940 -0.010167   \n",
      "460 -0.173734  0.056550  0.478723   0.150581  0.415584  0.167632  0.040963   \n",
      "431 -0.259204 -0.116527 -0.230685   0.023868  0.415584 -0.170829 -0.209037   \n",
      "420 -0.156640  0.059755 -0.085737  -0.060888  0.415584 -0.109291 -0.114307   \n",
      "220 -0.002794 -0.116527 -0.111521  -0.375013  0.415584  0.013786 -0.180807   \n",
      "194 -0.002794 -0.114925 -0.135911  -0.421167  0.415584 -0.109291 -0.138774   \n",
      "395  0.031395 -0.116527 -0.083646  -0.245223  0.415584  0.075325 -0.002325   \n",
      "137 -0.088264 -0.100502 -0.072496  -0.352356 -0.584416  0.198402 -0.150380   \n",
      "322  0.236523  0.075780 -0.119883   0.108903  0.415584 -0.278521 -0.052199   \n",
      "251 -0.088264 -0.116527  0.111476  -0.378650  0.415584  0.059940 -0.091095   \n",
      "305 -0.088264 -0.116527 -0.105946   0.031980 -0.584416 -0.216983  0.017123   \n",
      "299  0.236523  0.014883 -0.051591   0.248763 -0.584416 -0.324675  0.108717   \n",
      "374  0.014301 -0.116527 -0.163089   0.069462 -0.584416  0.259940  0.147299   \n",
      "101  0.236523 -0.114284 -0.049500   0.108623  0.415584 -0.001598  0.072958   \n",
      "409 -0.156640 -0.116527 -0.114308   0.043728  0.415584 -0.186214 -0.039025   \n",
      "178 -0.088264 -0.115245  0.242486   0.077294  0.415584  0.183017  0.006145   \n",
      "304 -0.019888 -0.078066 -0.136608  -0.511517  0.415584 -0.016983 -0.110857   \n",
      "207 -0.122452  0.011678  0.133078   0.151980 -0.584416  0.013786  0.073899   \n",
      "242  0.441651  0.017447  0.021580  -0.016132  0.415584 -0.124675  0.001439   \n",
      "\n",
      "      alcohol       age  chd  \n",
      "27   0.030951  0.371102    1  \n",
      "11  -0.115799 -0.057470    1  \n",
      "415  0.155348 -0.077878    1  \n",
      "382 -0.101803 -0.098286    1  \n",
      "106  0.030951  0.371102    1  \n",
      "215 -0.115799  0.391510    1  \n",
      "228 -0.101803  0.432326    1  \n",
      "18   0.246183  0.391510    1  \n",
      "17  -0.021431  0.207836    1  \n",
      "390 -0.028429 -0.077878    1  \n",
      "113 -0.115799  0.371102    1  \n",
      "32  -0.030535  0.330285    1  \n",
      "221 -0.089914  0.064979    1  \n",
      "123  0.053303  0.269061    1  \n",
      "333 -0.115799  0.330285    1  \n",
      "166  0.030951  0.248653    1  \n",
      "269  0.044946  0.044571    1  \n",
      "4    0.273766  0.126204    1  \n",
      "310 -0.059205  0.330285    1  \n",
      "346 -0.115799  0.207836    1  \n",
      "39   0.040054  0.309877    1  \n",
      "211 -0.007503 -0.057470    1  \n",
      "29  -0.059884  0.411918    1  \n",
      "387 -0.101803  0.207836    1  \n",
      "303 -0.115799  0.330285    1  \n",
      "31   0.053982  0.064979    1  \n",
      "183  0.072869 -0.077878    1  \n",
      "318 -0.098066  0.330285    1  \n",
      "345  0.420855 -0.343184    1  \n",
      "270 -0.115799  0.248653    1  \n",
      "..        ...       ...  ...  \n",
      "70  -0.111722 -0.567674    0  \n",
      "272 -0.066203 -0.384000    0  \n",
      "278 -0.115799 -0.547266    0  \n",
      "372  0.047052 -0.220735    0  \n",
      "279 -0.115799 -0.547266    0  \n",
      "250  0.513456  0.105796    0  \n",
      "366 -0.059205 -0.384000    0  \n",
      "283 -0.013074 -0.322776    0  \n",
      "210 -0.115799  0.350694    0  \n",
      "197  0.009957 -0.016653    0  \n",
      "331  0.065871  0.044571    0  \n",
      "79   0.373365 -0.077878    0  \n",
      "460  0.047052 -0.057470    0  \n",
      "431 -0.115799 -0.547266    0  \n",
      "420  0.497762  0.003755    0  \n",
      "220 -0.115799 -0.547266    0  \n",
      "194 -0.115799 -0.506449    0  \n",
      "395 -0.059001 -0.322776    0  \n",
      "137  0.038628 -0.302368    0  \n",
      "322  0.142779  0.371102    0  \n",
      "251 -0.115799 -0.526857    0  \n",
      "305 -0.002272  0.126204    0  \n",
      "299 -0.059884  0.207836    0  \n",
      "374  0.862529 -0.281960    0  \n",
      "101 -0.115799 -0.322776    0  \n",
      "409 -0.115799 -0.547266    0  \n",
      "178 -0.036106 -0.384000    0  \n",
      "304 -0.092631 -0.322776    0  \n",
      "207  0.111866  0.350694    0  \n",
      "242  0.447079 -0.037062    0  \n",
      "\n",
      "[320 rows x 10 columns]\n",
      "[27, 11, 415, 382, 106, 215, 228, 18, 17, 390, 113, 32, 221, 123, 333, 166, 269, 4, 310, 346, 39, 211, 29, 387, 303, 31, 183, 318, 345, 270, 107, 93, 397, 389, 149, 86, 46, 118, 159, 20, 446, 307, 52, 1, 77, 216, 154, 406, 370, 402, 260, 191, 403, 325, 43, 246, 342, 379, 461, 182, 255, 19, 235, 276, 413, 271, 240, 301, 249, 458, 80, 111, 167, 410, 128, 264, 334, 190, 140, 192, 341, 332, 148, 453, 155, 256, 295, 455, 83, 10, 424, 198, 125, 355, 91, 201, 364, 243, 274, 407, 293, 47, 252, 25, 33, 9, 116, 244, 53, 69, 280, 289, 422, 353, 391, 147, 227, 175, 0, 298, 449, 30, 135, 284, 189, 82, 141, 352, 232, 360, 258, 336, 398, 57, 229, 40, 348, 184, 98, 3, 312, 412, 35, 282, 226, 230, 223, 423, 81, 114, 385, 323, 185, 78, 7, 161, 454, 275, 132, 131]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:13: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n",
      "  del sys.path[0]\n"
     ]
    }
   ],
   "source": [
    "#重组数据集，保证label数量相等\n",
    "one_label_result = all_data[(all_data.chd == 1)]\n",
    "zero_label_result = all_data[(all_data.chd == 0)]\n",
    "\n",
    "one_label_length = len(one_label_result)\n",
    "zero_label_length = len(zero_label_result)\n",
    "\n",
    "small_len = one_label_length if one_label_length < zero_label_length else zero_label_length;\n",
    "\n",
    "one_index = random.sample(list(one_label_result.index.values), small_len)\n",
    "zero_index = random.sample(list(zero_label_result.index.values), small_len)\n",
    "\n",
    "new_data = pd.concat([one_label_result.ix[one_index], zero_label_result.ix[zero_index]])\n",
    "new_data.describe()\n",
    "print(new_data)\n",
    "# print(one_label_length)\n",
    "# print(zero_label_length)\n",
    "# print(small_len)\n",
    "print(one_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          sbp   tobacco       ldl  adiposity   famhist     typea   obesity  \\\n",
      "79  -0.259204 -0.103707  0.081510  -0.069559 -0.584416  0.059940 -0.010167   \n",
      "7   -0.207922  0.014242 -0.010476  -0.302286 -0.584416  0.136863 -0.092036   \n",
      "12  -0.173734 -0.116527 -0.199326  -0.429559  0.415584  0.090709 -0.140342   \n",
      "220 -0.002794 -0.116527 -0.111521  -0.375013  0.415584  0.013786 -0.180807   \n",
      "454  0.065583 -0.096014  0.005552   0.073098  0.415584  0.106094  0.064802   \n",
      "365 -0.054076 -0.093450 -0.025807  -0.164104  0.415584 -0.078521  0.002067   \n",
      "322  0.236523  0.075780 -0.119883   0.108903  0.415584 -0.278521 -0.052199   \n",
      "237  0.321993  0.068088  0.010430   0.019392 -0.584416 -0.109291  0.039394   \n",
      "426  0.031395 -0.074220  0.201371   0.127924 -0.584416  0.059940  0.160473   \n",
      "352  0.321993 -0.078066  0.246667   0.300791 -0.584416 -0.170829  0.055392   \n",
      "303 -0.088264 -0.116527  0.255727   0.095476 -0.584416  0.106094  0.025592   \n",
      "22   0.099771 -0.106912  0.114263   0.240091 -0.584416  0.136863 -0.044044   \n",
      "106 -0.259204 -0.068450 -0.028594  -0.011657  0.415584  0.198402 -0.117758   \n",
      "52   0.116865 -0.087681  0.305204   0.134917  0.415584  0.044555  0.081427   \n",
      "295  0.031395 -0.116527 -0.038350  -0.206062  0.415584  0.044555 -0.075098   \n",
      "449  0.048489  0.011678  0.020186   0.010441 -0.584416  0.059940  0.047236   \n",
      "299  0.236523  0.014883 -0.051591   0.248763 -0.584416 -0.324675  0.108717   \n",
      "99   0.219429  0.268088 -0.057862  -0.162706  0.415584 -0.032368 -0.081685   \n",
      "320  0.133959 -0.116527  0.004855   0.075616 -0.584416  0.044555 -0.011735   \n",
      "374  0.014301 -0.116527 -0.163089   0.069462 -0.584416  0.259940  0.147299   \n",
      "125 -0.130999  0.159114  0.448061   0.276175 -0.584416  0.259940  0.222581   \n",
      "148 -0.019888 -0.068450  0.091963   0.031700  0.415584  0.013786  0.104639   \n",
      "342  0.202335  0.085396  0.068967  -0.078230 -0.584416 -0.109291 -0.176101   \n",
      "251 -0.088264 -0.116527  0.111476  -0.378650  0.415584  0.059940 -0.091095   \n",
      "391  0.133959 -0.039604  0.061998   0.468903 -0.584416  0.090709  0.283121   \n",
      "301 -0.002794 -0.043771  0.116354   0.102469  0.415584  0.075325  0.130988   \n",
      "188 -0.036982  0.037319  0.128200   0.125406 -0.584416  0.029171 -0.041221   \n",
      "446  0.031395 -0.116527 -0.029291  -0.005223  0.415584 -0.093906  0.090210   \n",
      "427  0.065583 -0.079348 -0.171451   0.255196  0.415584 -0.047752  0.083623   \n",
      "55   0.116865  0.075460  0.226458   0.197854  0.415584 -0.124675  0.016496   \n",
      "..        ...       ...       ...        ...       ...       ...       ...   \n",
      "280 -0.276298 -0.081912 -0.025807   0.018833  0.415584  0.213786 -0.061923   \n",
      "278 -0.207922 -0.116527 -0.147061  -0.439629  0.415584 -0.124675 -0.255775   \n",
      "395  0.031395 -0.116527 -0.083646  -0.245223  0.415584  0.075325 -0.002325   \n",
      "451 -0.156640 -0.116527 -0.137305  -0.337251  0.415584  0.213786 -0.083881   \n",
      "380 -0.225016 -0.070373 -0.141486  -0.069559  0.415584  0.090709 -0.038711   \n",
      "256  0.065583  0.046293  0.159559   0.056035 -0.584416  0.152248  0.326722   \n",
      "331 -0.105358 -0.017168 -0.190267   0.211560 -0.584416  0.044555  0.081113   \n",
      "283 -0.036982 -0.020373 -0.109430  -0.209699  0.415584 -0.278521  0.010222   \n",
      "246 -0.019888 -0.103707 -0.057862  -0.120468 -0.584416  0.152248 -0.117444   \n",
      "356 -0.036982 -0.116527 -0.160998  -0.088580  0.415584 -0.016983  0.013986   \n",
      "250  0.168147  0.081229  0.235517   0.149462  0.415584 -0.109291  0.056333   \n",
      "444  0.031395 -0.020373 -0.073193  -0.008580  0.415584  0.106094  0.126596   \n",
      "10   0.578403  0.075780 -0.124761   0.191980  0.415584  0.290709  0.024024   \n",
      "309 -0.071170 -0.114925 -0.160301   0.079532 -0.584416  0.213786  0.151063   \n",
      "181  0.116865 -0.116527  0.091963   0.437574 -0.584416 -0.032368  0.448428   \n",
      "77   0.014301 -0.097296  0.057120   0.223308 -0.584416  0.075325  0.035944   \n",
      "367 -0.036982 -0.112681 -0.093402  -0.118230 -0.584416 -0.309291  0.007086   \n",
      "216  0.356181 -0.099861 -0.035563  -0.252496  0.415584  0.029171 -0.109288   \n",
      "406  0.356181  0.685075 -0.072496   0.355336 -0.584416  0.059940  0.141025   \n",
      "98  -0.139546 -0.013963  0.458514   0.278413 -0.584416  0.029171  0.032180   \n",
      "42  -0.156640 -0.116527 -0.255772  -0.262566  0.415584 -0.093906 -0.122149   \n",
      "173  0.185241 -0.116527 -0.161695   0.253238  0.415584 -0.078521  0.118754   \n",
      "240  0.014301  0.050139 -0.080859   0.107784  0.415584  0.259940  0.039081   \n",
      "91  -0.156640  0.002063 -0.050197   0.398693  0.415584  0.121479  0.141966   \n",
      "389  0.219429  0.062960 -0.109430   0.155896 -0.584416 -0.140060 -0.001697   \n",
      "307 -0.054076  0.152704 -0.081556  -0.328020  0.415584 -0.170829 -0.228799   \n",
      "210 -0.036982 -0.114925  0.229246   0.071140  0.415584 -0.078521  0.026220   \n",
      "340 -0.173734 -0.064604  0.297538  -0.103685  0.415584  0.090709 -0.004834   \n",
      "425 -0.054076 -0.116527 -0.100371  -0.106202  0.415584 -0.170829 -0.035261   \n",
      "110 -0.207922 -0.116527  0.227852  -0.105363  0.415584  0.198402 -0.016754   \n",
      "\n",
      "      alcohol       age  \n",
      "79   0.373365 -0.077878  \n",
      "7   -0.070143  0.309877  \n",
      "12  -0.115799 -0.526857  \n",
      "220 -0.115799 -0.547266  \n",
      "454 -0.059884 -0.077878  \n",
      "365  0.219618 -0.302368  \n",
      "322  0.142779  0.371102  \n",
      "237  0.016276  0.289469  \n",
      "426  0.379683 -0.200327  \n",
      "352 -0.036989  0.309877  \n",
      "303 -0.115799  0.330285  \n",
      "22  -0.115799  0.146612  \n",
      "106  0.030951  0.371102  \n",
      "52  -0.113285 -0.016653  \n",
      "295  0.025379 -0.016653  \n",
      "449  0.495656  0.105796  \n",
      "299 -0.059884  0.207836  \n",
      "99   0.018382 -0.077878  \n",
      "320  0.398978  0.330285  \n",
      "374  0.862529 -0.281960  \n",
      "125 -0.115799  0.330285  \n",
      "148 -0.017286 -0.200327  \n",
      "342  0.309094  0.207836  \n",
      "251 -0.115799 -0.526857  \n",
      "391 -0.028429  0.146612  \n",
      "301 -0.095892 -0.220735  \n",
      "188  0.044946  0.411918  \n",
      "446 -0.071434 -0.179919  \n",
      "427  0.189929  0.126204  \n",
      "55   0.565769  0.105796  \n",
      "..        ...       ...  \n",
      "280  0.004726 -0.302368  \n",
      "278 -0.115799 -0.547266  \n",
      "395 -0.059001 -0.322776  \n",
      "451 -0.108801 -0.506449  \n",
      "380 -0.115799  0.187428  \n",
      "256  0.050517 -0.118694  \n",
      "331  0.065871  0.044571  \n",
      "283 -0.013074 -0.322776  \n",
      "246 -0.115799  0.269061  \n",
      "356  0.167237 -0.384000  \n",
      "250  0.513456  0.105796  \n",
      "444  0.148350 -0.322776  \n",
      "10   0.265070  0.350694  \n",
      "309  0.158133 -0.179919  \n",
      "181 -0.115799  0.167020  \n",
      "77  -0.115799  0.248653  \n",
      "367 -0.019257 -0.261551  \n",
      "216 -0.015452  0.044571  \n",
      "406 -0.115799  0.371102  \n",
      "98  -0.115799  0.167020  \n",
      "42  -0.115799 -0.567674  \n",
      "173 -0.108801  0.371102  \n",
      "240  0.021235  0.044571  \n",
      "91  -0.115799  0.432326  \n",
      "389  0.177700  0.207836  \n",
      "307 -0.010968  0.330285  \n",
      "210 -0.115799  0.350694  \n",
      "340  0.028165 -0.057470  \n",
      "425  0.105752 -0.200327  \n",
      "110 -0.098882 -0.547266  \n",
      "\n",
      "[288 rows x 9 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:6: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# 数据分10份，拿一份做测试集，九份做训练集\n",
    "train_data_size = int(small_len * 2 * 0.9)\n",
    "test_data_size = int(small_len * 2 * 0.1)\n",
    "\n",
    "train_data_index = random.sample(list(new_data.index.values), train_data_size)\n",
    "train_data = new_data.ix[train_data_index]\n",
    "\n",
    "test_data_index = list(set(new_data.index.values).difference(set(train_data_index)))\n",
    "test_data = new_data.ix[test_data_index]\n",
    "\n",
    "train_label = train_data['chd']\n",
    "train_data = train_data.loc[:,raw_data.columns[:9]]\n",
    "\n",
    "test_label = test_data['chd']\n",
    "test_data = test_data.loc[:,raw_data.columns[:9]]\n",
    "# print(train_data.shape, train_label.shape)\n",
    "# print(test_data.shape, test_label.shape)\n",
    "print(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paramaters for the model\n",
    "# learning_rate = 0.01\n",
    "# batch_size = 16\n",
    "# n_epochs = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = tf.placeholder(dtype = np.float32, shape = [batch_size, 9], name='X')\n",
    "# Y = tf.placeholder(dtype = np.float32, shape = [batch_size, 2], name='Y')\n",
    "\n",
    "# W1 = tf.Variable(tf.random_normal([9, 20]), name='W1')\n",
    "# b1 = tf.Variable(tf.random_normal([1, 20]), name='b1')\n",
    "\n",
    "# W2 = tf.Variable(tf.random_normal([20, 10]), name='W2')\n",
    "# b2 = tf.Variable(tf.random_normal([1, 10]), name='b2')\n",
    "\n",
    "# W3 = tf.Variable(tf.random_normal([10, 2]), name='W3')\n",
    "# b3 = tf.Variable(tf.random_normal([1, 2]), name='b3')\n",
    "\n",
    "# Z1 = tf.matmul(X, W1) + b1\n",
    "# A1 = tf.nn.relu(Z1)\n",
    "# Z2 = tf.matmul(A1, W2) + b2\n",
    "# A2 = tf.nn.relu(Z2)\n",
    "# logits = tf.matmul(A2, W3) + b3\n",
    "\n",
    "# entropy = tf.nn.softmax_cross_entropy_with_logits(labels = Y, logits = logits)\n",
    "\n",
    "# loss = tf.reduce_mean(entropy)\n",
    "\n",
    "# optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "\n",
    "# def to_one_hotting(labels):\n",
    "#     return (np.arange(2) == labels[:,None]).astype(np.float32)\n",
    "\n",
    "# with tf.Session() as sess:\n",
    "#     start_time = time.time()\n",
    "#     sess.run(tf.global_variables_initializer())\t\n",
    "#     n_batches = int(len(train_data)/batch_size)\n",
    "#     for i in range(n_epochs): \n",
    "#         total_loss = 0\n",
    "\n",
    "#         for index in range(n_batches):\n",
    "#             X_batch = train_data[index*batch_size:(index+1)*batch_size].values\n",
    "#             Y_batch = train_label[index*batch_size:(index+1)*batch_size].values\n",
    "#             Y_batch = to_one_hotting(Y_batch)\n",
    "#             _, loss_batch, get_entropy, get_logits = sess.run([optimizer, loss, entropy, logits], feed_dict={X: X_batch, Y: Y_batch})\n",
    "#             total_loss += loss_batch\n",
    "            \n",
    "#         if i%100 == 0:\n",
    "#             print('Average loss epoch :{0}'.format(total_loss/n_batches))\n",
    "\n",
    "#     print('Total time: {0} seconds'.format(time.time() - start_time))\n",
    "\n",
    "#     print('Optimization Finished!')\n",
    "\n",
    "#     # test the model\n",
    "#     n_batches = int(len(test_data)/batch_size)\n",
    "#     total_correct_preds = 0\n",
    "#     for index in range(n_batches):\n",
    "#         X_batch = test_data[index*batch_size:(index+1)*batch_size].values\n",
    "#         Y_batch = test_label[index*batch_size:(index+1)*batch_size].values\n",
    "#         Y_batch = to_one_hotting(Y_batch)\n",
    "#         _, loss_batch, logits_batch = sess.run([optimizer, loss, logits], feed_dict={X: X_batch, Y:Y_batch}) \n",
    "#         preds = tf.nn.softmax(logits_batch)\n",
    "#         correct_preds = tf.equal(tf.argmax(preds, 1), tf.argmax(Y_batch, 1))\n",
    "#         accuracy = tf.reduce_sum(tf.cast(correct_preds, tf.float32)) # need numpy.count_nonzero(boolarr) :(\n",
    "#         total_correct_preds += sess.run(accuracy)\n",
    "\n",
    "#     print('Accuracy:',format(total_correct_preds/len(test_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: create_placeholders\n",
    "\n",
    "def create_placeholders(n_x, n_y):\n",
    "    \"\"\"\n",
    "    Creates the placeholders for the tensorflow session.\n",
    "    \n",
    "    Arguments:\n",
    "    n_x -- scalar, size of an image vector (num_px * num_px = 64 * 64 * 3 = 12288)\n",
    "    n_y -- scalar, number of classes (from 0 to 5, so -> 6)\n",
    "    \n",
    "    Returns:\n",
    "    X -- placeholder for the data input, of shape [n_x, None] and dtype \"float\"\n",
    "    Y -- placeholder for the input labels, of shape [n_y, None] and dtype \"float\"\n",
    "    \n",
    "    Tips:\n",
    "    - You will use None because it let's us be flexible on the number of examples you will for the placeholders.\n",
    "      In fact, the number of examples during test/train is different.\n",
    "    \"\"\"\n",
    "\n",
    "    ### START CODE HERE ### (approx. 2 lines)\n",
    "    X = tf.placeholder(shape=[n_x, None], dtype=tf.float32)\n",
    "    Y = tf.placeholder(shape=[n_y, None], dtype=tf.float32)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: initialize_parameters\n",
    "\n",
    "def initialize_parameters():\n",
    "    \"\"\"\n",
    "    Initializes parameters to build a neural network with tensorflow. The shapes are:\n",
    "                        W1 : [9, 20]\n",
    "                        b1 : [1, 20]\n",
    "                        W2 : [20, 10]\n",
    "                        b2 : [1, 10]\n",
    "                        W3 : [10, 2]\n",
    "                        b3 : [1 2]\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- a dictionary of tensors containing W1, b1, W2, b2, W3, b3\n",
    "    \"\"\"\n",
    "    \n",
    "#     tf.set_random_seed(1)                   # so that your \"random\" numbers match ours\n",
    "        \n",
    "    ### START CODE HERE ### (approx. 6 lines of code)\n",
    "#     W1 = tf.get_variable(\"W1\", [20, 288], initializer = tf.contrib.layers.xavier_initializer())\n",
    "#     b1 = tf.get_variable(\"b1\", [20,1], initializer = tf.contrib.layers.xavier_initializer())\n",
    "#     W2 = tf.get_variable(\"W2\", [10,20], initializer = tf.contrib.layers.xavier_initializer())\n",
    "#     b2 = tf.get_variable(\"b2\", [10,1], initializer = tf.contrib.layers.xavier_initializer())\n",
    "#     W3 = tf.get_variable(\"W3\", [2,10], initializer = tf.contrib.layers.xavier_initializer())\n",
    "#     b3 = tf.get_variable(\"b3\", [2,1], initializer = tf.contrib.layers.xavier_initializer())\n",
    "    \n",
    "    W1 = tf.Variable(tf.random_normal([20, 9]), name='W1')\n",
    "    b1 = tf.Variable(tf.random_normal([20, 1]), name='b1')\n",
    "\n",
    "    W2 = tf.Variable(tf.random_normal([10, 20]), name='W2')\n",
    "    b2 = tf.Variable(tf.random_normal([10, 1]), name='b2')\n",
    "\n",
    "    W3 = tf.Variable(tf.random_normal([1, 10]), name='W3')\n",
    "    b3 = tf.Variable(tf.random_normal([1, 1]), name='b3')\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2,\n",
    "                  \"W3\": W3,\n",
    "                  \"b3\": b3}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = <tf.Variable 'W1:0' shape=(20, 9) dtype=float32_ref>\n",
      "b1 = <tf.Variable 'b1:0' shape=(20, 1) dtype=float32_ref>\n",
      "W2 = <tf.Variable 'W2:0' shape=(10, 20) dtype=float32_ref>\n",
      "b2 = <tf.Variable 'b2:0' shape=(10, 1) dtype=float32_ref>\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "with tf.Session() as sess:\n",
    "    parameters = initialize_parameters()\n",
    "    print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "    print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "    print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "    print(\"b2 = \" + str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: forward_propagation\n",
    "\n",
    "def forward_propagation(X, parameters):\n",
    "    \"\"\"\n",
    "    Implements the forward propagation for the model: LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SOFTMAX\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input dataset placeholder, of shape (input size, number of examples)\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", \"W2\", \"b2\", \"W3\", \"b3\"\n",
    "                  the shapes are given in initialize_parameters\n",
    "\n",
    "    Returns:\n",
    "    Z3 -- the output of the last LINEAR unit\n",
    "    \"\"\"\n",
    "    \n",
    "    # Retrieve the parameters from the dictionary \"parameters\" \n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']\n",
    "    W3 = parameters['W3']\n",
    "    b3 = parameters['b3']\n",
    "    \n",
    "    ### START CODE HERE ### (approx. 5 lines)              # Numpy Equivalents:\n",
    "    Z1 = tf.add(tf.matmul(W1, X), b1)                                            # Z1 = np.dot(W1, X) + b1\n",
    "    A1 = tf.nn.relu(Z1)                                              # A1 = relu(Z1)\n",
    "    Z2 = tf.add(tf.matmul(W2, A1), b2)                                              # Z2 = np.dot(W2, a1) + b2\n",
    "    A2 = tf.nn.relu(Z2)                                              # A2 = relu(Z2)\n",
    "    Z3 = tf.add(tf.matmul(W3, A2), b3)                                              # Z3 = np.dot(W3,Z2) + b3\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return Z3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Dimensions must be equal, but are 9 and 288 for 'MatMul' (op: 'MatMul') with input shapes: [20,9], [288,?].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/common_shapes.py\u001b[0m in \u001b[0;36m_call_cpp_shape_fn_impl\u001b[0;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, require_shape_fn)\u001b[0m\n\u001b[1;32m    685\u001b[0m           \u001b[0mgraph_def_version\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_def_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shapes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_tensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 686\u001b[0;31m           input_tensors_as_shapes, status)\n\u001b[0m\u001b[1;32m    687\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    472\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 473\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    474\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Dimensions must be equal, but are 9 and 288 for 'MatMul' (op: 'MatMul') with input shapes: [20,9], [288,?].",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-f570b7fc64ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_placeholders\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m288\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m288\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mparameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitialize_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mZ3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_propagation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Z3 = \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mZ3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-9b89a3cbea3e>\u001b[0m in \u001b[0;36mforward_propagation\u001b[0;34m(X, parameters)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;31m### START CODE HERE ### (approx. 5 lines)              # Numpy Equivalents:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mZ1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb1\u001b[0m\u001b[0;34m)\u001b[0m                                            \u001b[0;31m# Z1 = np.dot(W1, X) + b1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0mA1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mZ1\u001b[0m\u001b[0;34m)\u001b[0m                                              \u001b[0;31m# A1 = relu(Z1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mZ2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb2\u001b[0m\u001b[0;34m)\u001b[0m                                              \u001b[0;31m# Z2 = np.dot(W2, a1) + b2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mmatmul\u001b[0;34m(a, b, transpose_a, transpose_b, adjoint_a, adjoint_b, a_is_sparse, b_is_sparse, name)\u001b[0m\n\u001b[1;32m   2014\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2015\u001b[0m       return gen_math_ops._mat_mul(\n\u001b[0;32m-> 2016\u001b[0;31m           a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)\n\u001b[0m\u001b[1;32m   2017\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2018\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36m_mat_mul\u001b[0;34m(a, b, transpose_a, transpose_b, name)\u001b[0m\n\u001b[1;32m   2514\u001b[0m     _, _, _op = _op_def_lib._apply_op_helper(\n\u001b[1;32m   2515\u001b[0m         \u001b[0;34m\"MatMul\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtranspose_a\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtranspose_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtranspose_b\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtranspose_b\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2516\u001b[0;31m         name=name)\n\u001b[0m\u001b[1;32m   2517\u001b[0m     \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2518\u001b[0m     \u001b[0m_inputs_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    785\u001b[0m         op = g.create_op(op_type_name, inputs, output_types, name=scope,\n\u001b[1;32m    786\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m                          op_def=op_def)\n\u001b[0m\u001b[1;32m    788\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0moutput_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_stateful\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mcreate_op\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes, compute_device)\u001b[0m\n\u001b[1;32m   3160\u001b[0m         op_def=op_def)\n\u001b[1;32m   3161\u001b[0m     self._create_op_helper(ret, compute_shapes=compute_shapes,\n\u001b[0;32m-> 3162\u001b[0;31m                            compute_device=compute_device)\n\u001b[0m\u001b[1;32m   3163\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_op_helper\u001b[0;34m(self, op, compute_shapes, compute_device)\u001b[0m\n\u001b[1;32m   3206\u001b[0m     \u001b[0;31m# compute_shapes argument.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3207\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_c_op\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mcompute_shapes\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3208\u001b[0;31m       \u001b[0mset_shapes_for_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3209\u001b[0m     \u001b[0;31m# TODO(b/XXXX): move to Operation.__init__ once _USE_C_API flag is removed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3210\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mset_shapes_for_outputs\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m   2425\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_set_shapes_for_outputs_c_api\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2426\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2427\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_set_shapes_for_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_set_shapes_for_outputs\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m   2398\u001b[0m       \u001b[0mshape_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2399\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2400\u001b[0;31m   \u001b[0mshapes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshape_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2401\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mshapes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2402\u001b[0m     raise RuntimeError(\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mcall_with_requiring\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m   2328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2329\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2330\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcall_cpp_shape_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequire_shape_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2331\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2332\u001b[0m   \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/common_shapes.py\u001b[0m in \u001b[0;36mcall_cpp_shape_fn\u001b[0;34m(op, require_shape_fn)\u001b[0m\n\u001b[1;32m    625\u001b[0m     res = _call_cpp_shape_fn_impl(op, input_tensors_needed,\n\u001b[1;32m    626\u001b[0m                                   \u001b[0minput_tensors_as_shapes_needed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 627\u001b[0;31m                                   require_shape_fn)\n\u001b[0m\u001b[1;32m    628\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m       \u001b[0;31m# Handles the case where _call_cpp_shape_fn_impl calls unknown_shape(op).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/common_shapes.py\u001b[0m in \u001b[0;36m_call_cpp_shape_fn_impl\u001b[0;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, require_shape_fn)\u001b[0m\n\u001b[1;32m    689\u001b[0m       \u001b[0mmissing_shape_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 691\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    692\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    693\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mmissing_shape_fn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Dimensions must be equal, but are 9 and 288 for 'MatMul' (op: 'MatMul') with input shapes: [20,9], [288,?]."
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "# print(train_data.shape)\n",
    "# print(train_label.shape[0])\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    X, Y = create_placeholders(288, 288)\n",
    "    parameters = initialize_parameters()\n",
    "    Z3 = forward_propagation(X, parameters)\n",
    "    print(\"Z3 = \" + str(Z3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: compute_cost \n",
    "\n",
    "def compute_cost(Z3, Y):\n",
    "    \"\"\"\n",
    "    Computes the cost\n",
    "    \n",
    "    Arguments:\n",
    "    Z3 -- output of forward propagation (output of the last LINEAR unit), of shape (6, number of examples)\n",
    "    Y -- \"true\" labels vector placeholder, same shape as Z3\n",
    "    \n",
    "    Returns:\n",
    "    cost - Tensor of the cost function\n",
    "    \"\"\"\n",
    "    \n",
    "    # to fit the tensorflow requirement for tf.nn.softmax_cross_entropy_with_logits(...,...)\n",
    "    logits = tf.transpose(Z3)\n",
    "    labels = tf.transpose(Y)\n",
    "    \n",
    "    ### START CODE HERE ### (1 line of code)\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits = logits, labels = labels))\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Dimensions must be equal, but are 9 and 288 for 'MatMul' (op: 'MatMul') with input shapes: [20,9], [288,?].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/common_shapes.py\u001b[0m in \u001b[0;36m_call_cpp_shape_fn_impl\u001b[0;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, require_shape_fn)\u001b[0m\n\u001b[1;32m    685\u001b[0m           \u001b[0mgraph_def_version\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_def_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shapes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_tensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 686\u001b[0;31m           input_tensors_as_shapes, status)\n\u001b[0m\u001b[1;32m    687\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    472\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 473\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    474\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Dimensions must be equal, but are 9 and 288 for 'MatMul' (op: 'MatMul') with input shapes: [20,9], [288,?].",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-0f4372b58854>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_placeholders\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m288\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m288\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mparameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitialize_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mZ3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_propagation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_cost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mZ3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cost = \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcost\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-9b89a3cbea3e>\u001b[0m in \u001b[0;36mforward_propagation\u001b[0;34m(X, parameters)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;31m### START CODE HERE ### (approx. 5 lines)              # Numpy Equivalents:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mZ1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb1\u001b[0m\u001b[0;34m)\u001b[0m                                            \u001b[0;31m# Z1 = np.dot(W1, X) + b1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0mA1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mZ1\u001b[0m\u001b[0;34m)\u001b[0m                                              \u001b[0;31m# A1 = relu(Z1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mZ2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb2\u001b[0m\u001b[0;34m)\u001b[0m                                              \u001b[0;31m# Z2 = np.dot(W2, a1) + b2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mmatmul\u001b[0;34m(a, b, transpose_a, transpose_b, adjoint_a, adjoint_b, a_is_sparse, b_is_sparse, name)\u001b[0m\n\u001b[1;32m   2014\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2015\u001b[0m       return gen_math_ops._mat_mul(\n\u001b[0;32m-> 2016\u001b[0;31m           a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)\n\u001b[0m\u001b[1;32m   2017\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2018\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36m_mat_mul\u001b[0;34m(a, b, transpose_a, transpose_b, name)\u001b[0m\n\u001b[1;32m   2514\u001b[0m     _, _, _op = _op_def_lib._apply_op_helper(\n\u001b[1;32m   2515\u001b[0m         \u001b[0;34m\"MatMul\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtranspose_a\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtranspose_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtranspose_b\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtranspose_b\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2516\u001b[0;31m         name=name)\n\u001b[0m\u001b[1;32m   2517\u001b[0m     \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2518\u001b[0m     \u001b[0m_inputs_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    785\u001b[0m         op = g.create_op(op_type_name, inputs, output_types, name=scope,\n\u001b[1;32m    786\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m                          op_def=op_def)\n\u001b[0m\u001b[1;32m    788\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0moutput_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_stateful\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mcreate_op\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes, compute_device)\u001b[0m\n\u001b[1;32m   3160\u001b[0m         op_def=op_def)\n\u001b[1;32m   3161\u001b[0m     self._create_op_helper(ret, compute_shapes=compute_shapes,\n\u001b[0;32m-> 3162\u001b[0;31m                            compute_device=compute_device)\n\u001b[0m\u001b[1;32m   3163\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_op_helper\u001b[0;34m(self, op, compute_shapes, compute_device)\u001b[0m\n\u001b[1;32m   3206\u001b[0m     \u001b[0;31m# compute_shapes argument.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3207\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_c_op\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mcompute_shapes\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3208\u001b[0;31m       \u001b[0mset_shapes_for_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3209\u001b[0m     \u001b[0;31m# TODO(b/XXXX): move to Operation.__init__ once _USE_C_API flag is removed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3210\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mset_shapes_for_outputs\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m   2425\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_set_shapes_for_outputs_c_api\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2426\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2427\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_set_shapes_for_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_set_shapes_for_outputs\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m   2398\u001b[0m       \u001b[0mshape_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2399\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2400\u001b[0;31m   \u001b[0mshapes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshape_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2401\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mshapes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2402\u001b[0m     raise RuntimeError(\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mcall_with_requiring\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m   2328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2329\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2330\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcall_cpp_shape_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequire_shape_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2331\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2332\u001b[0m   \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/common_shapes.py\u001b[0m in \u001b[0;36mcall_cpp_shape_fn\u001b[0;34m(op, require_shape_fn)\u001b[0m\n\u001b[1;32m    625\u001b[0m     res = _call_cpp_shape_fn_impl(op, input_tensors_needed,\n\u001b[1;32m    626\u001b[0m                                   \u001b[0minput_tensors_as_shapes_needed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 627\u001b[0;31m                                   require_shape_fn)\n\u001b[0m\u001b[1;32m    628\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m       \u001b[0;31m# Handles the case where _call_cpp_shape_fn_impl calls unknown_shape(op).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/common_shapes.py\u001b[0m in \u001b[0;36m_call_cpp_shape_fn_impl\u001b[0;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, require_shape_fn)\u001b[0m\n\u001b[1;32m    689\u001b[0m       \u001b[0mmissing_shape_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 691\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    692\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    693\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mmissing_shape_fn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Dimensions must be equal, but are 9 and 288 for 'MatMul' (op: 'MatMul') with input shapes: [20,9], [288,?]."
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    X, Y = create_placeholders(288, 288)\n",
    "    parameters = initialize_parameters()\n",
    "    Z3 = forward_propagation(X, parameters)\n",
    "    cost = compute_cost(Z3, Y)\n",
    "    print(\"cost = \" + str(cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X_train, Y_train, X_test, Y_test, learning_rate = 0.0001,\n",
    "          num_epochs = 1500, minibatch_size = 32, print_cost = True):\n",
    "    \"\"\"\n",
    "    Implements a three-layer tensorflow neural network: LINEAR->RELU->LINEAR->RELU->LINEAR->SOFTMAX.\n",
    "    \n",
    "    Arguments:\n",
    "    X_train -- training set, of shape (input size = 12288, number of training examples = 1080)\n",
    "    Y_train -- test set, of shape (output size = 6, number of training examples = 1080)\n",
    "    X_test -- training set, of shape (input size = 12288, number of training examples = 120)\n",
    "    Y_test -- test set, of shape (output size = 6, number of test examples = 120)\n",
    "    learning_rate -- learning rate of the optimization\n",
    "    num_epochs -- number of epochs of the optimization loop\n",
    "    minibatch_size -- size of a minibatch\n",
    "    print_cost -- True to print the cost every 100 epochs\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "    \n",
    "#     ops.reset_default_graph()                         # to be able to rerun the model without overwriting tf variables\n",
    "    (n_x, m) = X_train.shape                          # (n_x: input size, m : number of examples in the train set)\n",
    "    n_y = Y_train.shape[0]                            # n_y : output size\n",
    "    costs = []       # To keep track of the cost\n",
    "    \n",
    "    # Create Placeholders of shape (n_x, n_y)\n",
    "    ### START CODE HERE ### (1 line)\n",
    "\n",
    "    X = tf.placeholder(dtype = np.float32, shape = [9, None])\n",
    "    Y = tf.placeholder(dtype = np.float32, shape = [1, None])    \n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    # Initialize parameters\n",
    "    ### START CODE HERE ### (1 line)\n",
    "    parameters = initialize_parameters()\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Forward propagation: Build the forward propagation in the tensorflow graph\n",
    "    ### START CODE HERE ### (1 line)\n",
    "    Z3 = forward_propagation(X, parameters)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Cost function: Add cost function to tensorflow graph\n",
    "    ### START CODE HERE ### (1 line)\n",
    "    cost = compute_cost(Z3, Y)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Backpropagation: Define the tensorflow optimizer. Use an AdamOptimizer.\n",
    "    ### START CODE HERE ### (1 line)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Initialize all the variables\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    # Start the session to compute the tensorflow graph\n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        # Run the initialization\n",
    "        sess.run(init)\n",
    "        \n",
    "        # Do the training loop\n",
    "        for epoch in range(num_epochs):\n",
    "\n",
    "            epoch_cost = 0.                       # Defines a cost related to an epoch\n",
    "            num_minibatches = int(m / minibatch_size) # number of minibatches of size minibatch_size in the train set\n",
    "            print(minibatch_size)\n",
    "            print(m)\n",
    "            print(m / minibatch_size)\n",
    "        \n",
    "            minibatches = random_mini_batches(X_train, Y_train, minibatch_size)\n",
    "            \n",
    "            \n",
    "            \n",
    "            for minibatch in minibatches:\n",
    "\n",
    "                # Select a minibatch\n",
    "                (minibatch_X, minibatch_Y) = minibatch\n",
    "                \n",
    "                # IMPORTANT: The line that runs the graph on a minibatch.\n",
    "                # Run the session to execute the \"optimizer\" and the \"cost\", the feedict should contain a minibatch for (X,Y).\n",
    "                ### START CODE HERE ### (1 line)\n",
    "                _ , minibatch_cost = sess.run([optimizer, cost], feed_dict={X: minibatch_X, Y: minibatch_Y})\n",
    "                ### END CODE HERE ###\n",
    "                \n",
    "                epoch_cost += minibatch_cost / num_minibatches\n",
    "\n",
    "            # Print the cost every epoch\n",
    "            if print_cost == True and epoch % 100 == 0:\n",
    "                print (\"Cost after epoch %i: %f\" % (epoch, epoch_cost))\n",
    "            if print_cost == True and epoch % 5 == 0:\n",
    "                costs.append(epoch_cost)\n",
    "                \n",
    "        # plot the cost\n",
    "        plt.plot(np.squeeze(costs))\n",
    "        plt.ylabel('cost')\n",
    "        plt.xlabel('iterations (per tens)')\n",
    "        plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "        plt.show()\n",
    "\n",
    "        # lets save the parameters in a variable\n",
    "        parameters = sess.run(parameters)\n",
    "        print (\"Parameters have been trained!\")\n",
    "\n",
    "        # Calculate the correct predictions\n",
    "        correct_prediction = tf.equal(tf.argmax(Z3), tf.argmax(Y))\n",
    "\n",
    "        # Calculate accuracy on the test set\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "\n",
    "        print (\"Train Accuracy:\", accuracy.eval({X: X_train, Y: Y_train}))\n",
    "        print (\"Test Accuracy:\", accuracy.eval({X: X_test, Y: Y_test}))\n",
    "        \n",
    "        return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "9\n",
      "0.28125\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'slice'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-c4573f0874c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mparameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-15-c5d513087acd>\u001b[0m in \u001b[0;36mmodel\u001b[0;34m(X_train, Y_train, X_test, Y_test, learning_rate, num_epochs, minibatch_size, print_cost)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mminibatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0mminibatches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom_mini_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminibatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/notebooks/predict_disease/tf_utils.py\u001b[0m in \u001b[0;36mrandom_mini_batches\u001b[0;34m(X, Y, mini_batch_size, seed)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;31m# Step 1: Shuffle (X, Y)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mpermutation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermutation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0mshuffled_X\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpermutation\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m     \u001b[0mshuffled_Y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpermutation\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2137\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2138\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2139\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2141\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_getitem_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_getitem_column\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2144\u001b[0m         \u001b[0;31m# get column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2145\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2146\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_item_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2148\u001b[0m         \u001b[0;31m# duplicate columns & possible reduce dimensionality\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_get_item_cache\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m   1838\u001b[0m         \u001b[0;34m\"\"\"Return the cached item, item represents a label indexer.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1839\u001b[0m         \u001b[0mcache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_item_cache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1840\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1841\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1842\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'slice'"
     ]
    }
   ],
   "source": [
    "parameters = model(train_data, train_label, test_data, test_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
