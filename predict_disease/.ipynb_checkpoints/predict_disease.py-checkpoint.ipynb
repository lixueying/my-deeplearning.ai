{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from __future__ import print_function\n",
    "import collections\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "from matplotlib import pylab\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "from sklearn.manifold import TSNE\n",
    "from tf_utils import random_mini_batches\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['sbp', 'tobacco', 'ldl', 'adiposity', 'famhist', 'typea', 'obesity',\n",
      "       'alcohol', 'age'],\n",
      "      dtype='object')\n",
      "     sbp  tobacco    ldl  adiposity  famhist  typea  obesity  alcohol  age\n",
      "0    160    12.00   5.73      23.11  Present     49    25.30    97.20   52\n",
      "1    144     0.01   4.41      28.61   Absent     55    28.87     2.06   63\n",
      "2    118     0.08   3.48      32.28  Present     52    29.14     3.81   46\n",
      "3    170     7.50   6.41      38.03  Present     51    31.99    24.26   58\n",
      "4    134    13.60   3.50      27.78  Present     60    25.99    57.34   49\n",
      "5    132     6.20   6.47      36.21  Present     62    30.77    14.14   45\n",
      "6    142     4.05   3.38      16.20   Absent     59    20.81     2.62   38\n",
      "7    114     4.08   4.59      14.60  Present     62    23.11     6.72   58\n",
      "8    114     0.00   3.83      19.40  Present     49    24.86     2.49   29\n",
      "9    132     0.00   5.80      30.96  Present     69    30.11     0.00   53\n",
      "10   206     6.00   2.95      32.27   Absent     72    26.81    56.06   60\n",
      "11   134    14.10   4.44      22.39  Present     65    23.09     0.00   40\n",
      "12   118     0.00   1.88      10.05   Absent     59    21.57     0.00   17\n",
      "13   132     0.00   1.87      17.21   Absent     49    23.63     0.97   15\n",
      "14   112     9.65   2.29      17.20  Present     54    23.53     0.68   53\n",
      "15   117     1.53   2.44      28.95  Present     35    25.89    30.03   46\n",
      "16   120     7.50  15.33      22.00   Absent     60    25.31    34.49   49\n",
      "17   146    10.50   8.29      35.36  Present     78    32.73    13.89   53\n",
      "18   158     2.60   7.46      34.07  Present     61    29.30    53.28   62\n",
      "19   124    14.00   6.23      35.96  Present     45    30.09     0.00   59\n",
      "20   106     1.61   1.74      12.32   Absent     74    20.92    13.37   20\n",
      "21   132     7.90   2.85      26.50  Present     51    26.16    25.71   44\n",
      "22   150     0.30   6.38      33.99  Present     62    24.64     0.00   50\n",
      "23   138     0.60   3.81      28.66   Absent     54    28.70     1.46   58\n",
      "24   142    18.20   4.34      24.38   Absent     61    26.19     0.00   50\n",
      "25   124     4.00  12.42      31.29  Present     54    23.23     2.06   42\n",
      "26   118     6.00   9.65      33.91   Absent     60    38.80     0.00   48\n",
      "27   145     9.10   5.24      27.55   Absent     59    20.96    21.60   61\n",
      "28   144     4.09   5.55      31.40  Present     60    29.43     5.55   56\n",
      "29   146     0.00   6.62      25.69   Absent     60    28.07     8.23   63\n",
      "..   ...      ...    ...        ...      ...    ...      ...      ...  ...\n",
      "432  136     0.00   4.00      19.06   Absent     40    21.94     2.06   16\n",
      "433  120     0.00   2.46      13.39   Absent     47    22.01     0.51   18\n",
      "434  132     0.00   3.55       8.66  Present     61    18.50     3.87   16\n",
      "435  136     0.00   1.77      20.37   Absent     45    21.51     2.06   16\n",
      "436  138     0.00   1.86      18.35  Present     59    25.38     6.51   17\n",
      "437  138     0.06   4.15      20.66   Absent     49    22.59     2.49   16\n",
      "438  130     1.22   3.30      13.65   Absent     50    21.40     3.81   31\n",
      "439  130     4.00   2.40      17.42   Absent     60    22.05     0.00   40\n",
      "440  110     0.00   7.14      28.28   Absent     57    29.00     0.00   32\n",
      "441  120     0.00   3.98      13.19  Present     47    21.89     0.00   16\n",
      "442  166     6.00   8.80      37.89   Absent     39    28.70    43.20   52\n",
      "443  134     0.57   4.75      23.07   Absent     67    26.33     0.00   37\n",
      "444  142     3.00   3.69      25.10   Absent     60    30.08    38.88   27\n",
      "445  136     2.80   2.53       9.28  Present     61    20.70     4.55   25\n",
      "446  142     0.00   4.32      25.22   Absent     47    28.92     6.53   34\n",
      "447  130     0.00   1.88      12.51  Present     52    20.28     0.00   17\n",
      "448  124     1.80   3.74      16.64  Present     42    22.26    10.49   20\n",
      "449  144     4.00   5.03      25.78  Present     57    27.55    90.00   48\n",
      "450  136     1.81   3.31       6.74   Absent     63    19.57    24.94   24\n",
      "451  120     0.00   2.77      13.35   Absent     67    23.37     1.03   18\n",
      "452  154     5.53   3.20      28.81  Present     61    26.15    42.79   42\n",
      "453  124     1.60   7.22      39.68  Present     36    31.50     0.00   51\n",
      "454  146     0.64   4.82      28.02   Absent     60    28.11     8.23   39\n",
      "455  128     2.24   2.83      26.48   Absent     48    23.96    47.42   27\n",
      "456  170     0.40   4.11      42.06  Present     56    33.10     2.06   57\n",
      "457  214     0.40   5.98      31.72   Absent     64    28.45     0.00   58\n",
      "458  182     4.20   4.41      32.10   Absent     52    28.61    18.72   52\n",
      "459  108     3.00   1.59      15.23   Absent     40    20.09    26.64   55\n",
      "460  118     5.40  11.61      30.79   Absent     64    27.35    23.97   40\n",
      "461  132     0.00   4.82      33.41  Present     62    14.70     0.00   46\n",
      "\n",
      "[462 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "# download data\n",
    "raw_data = pd.read_csv('heart.csv')\n",
    "\n",
    "print(raw_data.columns[:9])\n",
    "print(raw_data.loc[:,raw_data.columns[:9]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          sbp   tobacco       ldl  adiposity   famhist     typea   obesity  \\\n",
      "0    0.185241  0.268088  0.068967  -0.064244 -0.584416 -0.063137 -0.023341   \n",
      "1    0.048489 -0.116207 -0.023019   0.089602  0.415584  0.029171  0.088641   \n",
      "2   -0.173734 -0.113963 -0.087828   0.192259 -0.584416 -0.016983  0.097111   \n",
      "3    0.270711  0.123857  0.116354   0.353098 -0.584416 -0.032368  0.186508   \n",
      "4   -0.036982  0.319370 -0.086434   0.066385 -0.584416  0.106094 -0.001697   \n",
      "5   -0.054076  0.082191  0.120535   0.302189 -0.584416  0.136863  0.148240   \n",
      "6    0.031395  0.013280 -0.094796  -0.257531  0.415584  0.090709 -0.164182   \n",
      "7   -0.207922  0.014242 -0.010476  -0.302286 -0.584416  0.136863 -0.092036   \n",
      "8   -0.207922 -0.116527 -0.063437  -0.168020 -0.584416 -0.063137 -0.037143   \n",
      "9   -0.054076 -0.116527  0.073845   0.155336 -0.584416  0.244555  0.127537   \n",
      "10   0.578403  0.075780 -0.124761   0.191980  0.415584  0.290709  0.024024   \n",
      "11  -0.036982  0.335396 -0.020929  -0.084384 -0.584416  0.183017 -0.092664   \n",
      "12  -0.173734 -0.116527 -0.199326  -0.429559  0.415584  0.090709 -0.140342   \n",
      "13  -0.054076 -0.116527 -0.200023  -0.229279  0.415584 -0.063137 -0.075725   \n",
      "14  -0.225016  0.192768 -0.170754  -0.229559 -0.584416  0.013786 -0.078862   \n",
      "15  -0.182281 -0.067489 -0.160301   0.099112 -0.584416 -0.278521 -0.004834   \n",
      "16  -0.156640  0.123857  0.737956  -0.095293  0.415584  0.106094 -0.023027   \n",
      "17   0.065583  0.220011  0.247364   0.278413 -0.584416  0.383017  0.209720   \n",
      "18   0.168147 -0.033194  0.189524   0.242329 -0.584416  0.121479  0.102129   \n",
      "19  -0.122452  0.332191  0.103810   0.295196 -0.584416 -0.124675  0.126910   \n",
      "20  -0.276298 -0.064925 -0.209082  -0.366062  0.415584  0.321479 -0.160731   \n",
      "21  -0.054076  0.136678 -0.131730   0.030581 -0.584416 -0.032368  0.003635   \n",
      "22   0.099771 -0.106912  0.114263   0.240091 -0.584416  0.136863 -0.044044   \n",
      "23  -0.002794 -0.097296 -0.064831   0.091001  0.415584  0.013786  0.083309   \n",
      "24   0.031395  0.466806 -0.027897  -0.028720  0.415584  0.121479  0.004576   \n",
      "25  -0.122452  0.011678  0.535169   0.164567 -0.584416  0.013786 -0.088272   \n",
      "26  -0.173734  0.075780  0.342138   0.237854  0.415584  0.106094  0.400122   \n",
      "27   0.057036  0.175139  0.034821   0.059952  0.415584  0.090709 -0.159477   \n",
      "28   0.048489  0.014563  0.056423   0.167644 -0.584416  0.106094  0.106207   \n",
      "29   0.065583 -0.116527  0.130988   0.007924  0.415584  0.106094  0.063547   \n",
      "..        ...       ...       ...        ...       ...       ...       ...   \n",
      "432 -0.019888 -0.116527 -0.051591  -0.177531  0.415584 -0.201598 -0.128736   \n",
      "433 -0.156640 -0.116527 -0.158908  -0.336132  0.415584 -0.093906 -0.126541   \n",
      "434 -0.054076 -0.116527 -0.082949  -0.468440 -0.584416  0.121479 -0.236641   \n",
      "435 -0.019888 -0.116527 -0.206991  -0.140888  0.415584 -0.124675 -0.142224   \n",
      "436 -0.002794 -0.116527 -0.200719  -0.197391 -0.584416  0.090709 -0.020832   \n",
      "437 -0.002794 -0.114604 -0.041138  -0.132776  0.415584 -0.063137 -0.108347   \n",
      "438 -0.071170 -0.077425 -0.100371  -0.328860  0.415584 -0.047752 -0.145675   \n",
      "439 -0.071170  0.011678 -0.163089  -0.223405  0.415584  0.106094 -0.125286   \n",
      "440 -0.242110 -0.116527  0.167225   0.080371  0.415584  0.059940  0.092719   \n",
      "441 -0.156640 -0.116527 -0.052984  -0.341727 -0.584416 -0.093906 -0.130305   \n",
      "442  0.236523  0.075780  0.282904   0.349182  0.415584 -0.216983  0.083309   \n",
      "443 -0.036982 -0.098258  0.000674  -0.065363  0.415584  0.213786  0.008968   \n",
      "444  0.031395 -0.020373 -0.073193  -0.008580  0.415584  0.106094  0.126596   \n",
      "445 -0.019888 -0.026784 -0.154030  -0.451097 -0.584416  0.121479 -0.167632   \n",
      "446  0.031395 -0.116527 -0.029291  -0.005223  0.415584 -0.093906  0.090210   \n",
      "447 -0.071170 -0.116527 -0.199326  -0.360748 -0.584416 -0.016983 -0.180807   \n",
      "448 -0.122452 -0.058835 -0.069709  -0.245223 -0.584416 -0.170829 -0.118699   \n",
      "449  0.048489  0.011678  0.020186   0.010441 -0.584416  0.059940  0.047236   \n",
      "450 -0.019888 -0.058514 -0.099674  -0.522146  0.415584  0.152248 -0.203078   \n",
      "451 -0.156640 -0.116527 -0.137305  -0.337251  0.415584  0.213786 -0.083881   \n",
      "452  0.133959  0.060716 -0.107340   0.095196 -0.584416  0.121479  0.003321   \n",
      "453 -0.122452 -0.065245  0.172800   0.399252 -0.584416 -0.263137  0.171138   \n",
      "454  0.065583 -0.096014  0.005552   0.073098  0.415584  0.106094  0.064802   \n",
      "455 -0.088264 -0.044732 -0.133124   0.030021  0.415584 -0.078521 -0.065374   \n",
      "456  0.270711 -0.103707 -0.043925   0.465826 -0.584416  0.044555  0.221326   \n",
      "457  0.646779 -0.103707  0.086389   0.176595  0.415584  0.167632  0.075467   \n",
      "458  0.373275  0.018088 -0.023019   0.187224  0.415584 -0.016983  0.080486   \n",
      "459 -0.259204 -0.020373 -0.219535  -0.284664  0.415584 -0.201598 -0.186766   \n",
      "460 -0.173734  0.056550  0.478723   0.150581  0.415584  0.167632  0.040963   \n",
      "461 -0.054076 -0.116527  0.005552   0.223868 -0.584416  0.136863 -0.355838   \n",
      "\n",
      "      alcohol       age  chd  \n",
      "0    0.544572  0.187428    1  \n",
      "1   -0.101803  0.411918    1  \n",
      "2   -0.089914  0.064979    0  \n",
      "3    0.049022  0.309877    1  \n",
      "4    0.273766  0.126204    1  \n",
      "5   -0.019732  0.044571    0  \n",
      "6   -0.097998 -0.098286    0  \n",
      "7   -0.070143  0.309877    1  \n",
      "8   -0.098882 -0.281960    0  \n",
      "9   -0.115799  0.207836    1  \n",
      "10   0.265070  0.350694    1  \n",
      "11  -0.115799 -0.057470    1  \n",
      "12  -0.115799 -0.526857    0  \n",
      "13  -0.109208 -0.567674    0  \n",
      "14  -0.111179  0.207836    0  \n",
      "15   0.088223  0.064979    0  \n",
      "16   0.118524  0.126204    0  \n",
      "17  -0.021431  0.207836    1  \n",
      "18   0.246183  0.391510    1  \n",
      "19  -0.115799  0.330285    1  \n",
      "20  -0.024964 -0.465633    1  \n",
      "21   0.058874  0.024163    0  \n",
      "22  -0.115799  0.146612    0  \n",
      "23  -0.105879  0.309877    0  \n",
      "24  -0.115799  0.146612    0  \n",
      "25  -0.101803 -0.016653    1  \n",
      "26  -0.115799  0.105796    0  \n",
      "27   0.030951  0.371102    1  \n",
      "28  -0.078092  0.269061    0  \n",
      "29  -0.059884  0.411918    1  \n",
      "..        ...       ...  ...  \n",
      "432 -0.101803 -0.547266    0  \n",
      "433 -0.112334 -0.506449    0  \n",
      "434 -0.089506 -0.547266    0  \n",
      "435 -0.101803 -0.547266    0  \n",
      "436 -0.071570 -0.526857    0  \n",
      "437 -0.098882 -0.547266    0  \n",
      "438 -0.089914 -0.241143    0  \n",
      "439 -0.115799 -0.057470    0  \n",
      "440 -0.115799 -0.220735    0  \n",
      "441 -0.115799 -0.547266    0  \n",
      "442  0.177700  0.187428    0  \n",
      "443 -0.115799 -0.118694    0  \n",
      "444  0.148350 -0.322776    0  \n",
      "445 -0.084886 -0.363592    0  \n",
      "446 -0.071434 -0.179919    1  \n",
      "447 -0.115799 -0.526857    0  \n",
      "448 -0.044530 -0.465633    0  \n",
      "449  0.495656  0.105796    1  \n",
      "450  0.053642 -0.384000    0  \n",
      "451 -0.108801 -0.506449    0  \n",
      "452  0.174914 -0.016653    0  \n",
      "453 -0.115799  0.167020    1  \n",
      "454 -0.059884 -0.077878    1  \n",
      "455  0.206370 -0.322776    1  \n",
      "456 -0.101803  0.289469    0  \n",
      "457 -0.115799  0.309877    0  \n",
      "458  0.011384  0.187428    1  \n",
      "459  0.065192  0.248653    0  \n",
      "460  0.047052 -0.057470    0  \n",
      "461 -0.115799  0.064979    1  \n",
      "\n",
      "[462 rows x 10 columns]\n"
     ]
    }
   ],
   "source": [
    "label = raw_data['chd']\n",
    "# data = raw_data.loc[:,raw_data.columns[:9]].replace(['Present', 'Absent'],[0, 1])\n",
    "# data = data.apply(lambda x: (x - np.mean(x)) / (np.max(x) - np.min(x)), axis=0)\n",
    "# all_data = data\n",
    "# all_data['chd'] = label\n",
    "# all_data.describe()\n",
    "# print(all_data)\n",
    "\n",
    "data = raw_data.loc[:,raw_data.columns[:9]].replace(['Present', 'Absent'],[0, 1])\n",
    "# 归一化data\n",
    "data = data.apply(lambda x: (x - np.mean(x)) / (np.max(x) - np.min(x)), axis=0)\n",
    "\n",
    "all_data = data\n",
    "all_data['chd'] = label\n",
    "# all_data.describe()\n",
    "\n",
    "print(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          sbp   tobacco       ldl  adiposity   famhist     typea   obesity  \\\n",
      "198  0.185241 -0.020373  0.310082   0.029742 -0.584416 -0.216983  0.069193   \n",
      "81   0.014301  0.144370  0.013218   0.477854  0.415584 -0.001598  0.617186   \n",
      "69  -0.139546 -0.116527 -0.095493  -0.260328  0.415584  0.213786 -0.156340   \n",
      "175 -0.139546  0.011678  0.034821   0.069462 -0.584416 -0.124675  0.014927   \n",
      "128 -0.036982  0.165524  0.186040   0.040091  0.415584 -0.278521  0.106521   \n",
      "275 -0.019888  0.165524 -0.003507   0.298273 -0.584416 -0.232368  0.016182   \n",
      "352  0.321993 -0.078066  0.246667   0.300791 -0.584416 -0.170829  0.055392   \n",
      "106 -0.259204 -0.068450 -0.028594  -0.011657  0.415584  0.198402 -0.117758   \n",
      "113  0.304899 -0.116527  0.259211   0.271140 -0.584416 -0.278521 -0.024282   \n",
      "191 -0.088264 -0.094091  0.011127   0.336035 -0.584416  0.290709  0.310411   \n",
      "228 -0.002794 -0.052425  0.025761   0.167644 -0.584416 -0.063137  0.037826   \n",
      "83   0.082677  0.274498 -0.066225   0.244567  0.415584  0.059940  0.010536   \n",
      "135 -0.139546  0.017447  0.300326   0.108063 -0.584416 -0.140060 -0.062551   \n",
      "303 -0.088264 -0.116527  0.255727   0.095476 -0.584416  0.106094  0.025592   \n",
      "17   0.065583  0.220011  0.247364   0.278413 -0.584416  0.383017  0.209720   \n",
      "155 -0.002794  0.165524 -0.112915  -0.083825 -0.584416  0.152248 -0.085135   \n",
      "190  0.031395 -0.116527 -0.071103   0.007644  0.415584 -0.078521 -0.052513   \n",
      "223 -0.019888  0.123857  0.184646   0.073658 -0.584416 -0.047752 -0.032438   \n",
      "413  0.663873 -0.087040 -0.144970  -0.155433 -0.584416 -0.063137 -0.171396   \n",
      "289 -0.225016 -0.097296  0.037608   0.008483  0.415584  0.029171  0.030611   \n",
      "19  -0.122452  0.332191  0.103810   0.295196 -0.584416 -0.124675  0.126910   \n",
      "3    0.270711  0.123857  0.116354   0.353098 -0.584416 -0.032368  0.186508   \n",
      "346  0.133959  0.027704  0.000674  -0.052776 -0.584416 -0.155445 -0.008912   \n",
      "454  0.065583 -0.096014  0.005552   0.073098  0.415584  0.106094  0.064802   \n",
      "284  0.082677  0.364242  0.016702   0.322609 -0.584416  0.290709  0.181490   \n",
      "310  0.304899 -0.004348  0.036214  -0.096132 -0.584416 -0.263137 -0.125600   \n",
      "345  0.510027 -0.099861  0.498235   0.063588 -0.584416 -0.078521  0.073899   \n",
      "221  0.219429 -0.100502  0.153984   0.398133 -0.584416 -0.093906  0.492970   \n",
      "39  -0.019888  0.242447  0.074542   0.180231 -0.584416  0.336863  0.051314   \n",
      "167  0.185241 -0.067809  0.235517   0.108903 -0.584416  0.013786 -0.005461   \n",
      "..        ...       ...       ...        ...       ...       ...       ...   \n",
      "136 -0.190828 -0.029989 -0.073193  -0.332496  0.415584  0.029171 -0.154144   \n",
      "174 -0.088264 -0.116527 -0.155423   0.110301 -0.584416 -0.001598 -0.125286   \n",
      "290 -0.225016 -0.116527 -0.211172  -0.264244  0.415584 -0.170829 -0.125913   \n",
      "236 -0.105358  0.059755 -0.066922   0.244567  0.415584  0.029171  0.088014   \n",
      "428 -0.054076  0.114242 -0.075981  -0.230678 -0.584416  0.044555 -0.087645   \n",
      "363  0.236523 -0.097296 -0.161695   0.241210 -0.584416 -0.001598  0.028729   \n",
      "102 -0.173734 -0.116527 -0.027897   0.131840 -0.584416 -0.016983  0.192468   \n",
      "372 -0.088264 -0.065245  0.046667   0.108903  0.415584  0.229171  0.104639   \n",
      "94   0.475839 -0.062040  0.110082   0.231140  0.415584 -0.093906  0.129106   \n",
      "324 -0.019888 -0.062040 -0.084343  -0.147601  0.415584  0.044555 -0.207155   \n",
      "24   0.031395  0.466806 -0.027897  -0.028720  0.415584  0.121479  0.004576   \n",
      "268 -0.207922 -0.113322 -0.055075  -0.266202 -0.584416  0.059940 -0.179866   \n",
      "386  0.048489 -0.116527 -0.039744   0.118133 -0.584416 -0.016983 -0.132187   \n",
      "311 -0.207922  0.191165 -0.155423   0.105546  0.415584 -0.063137 -0.011735   \n",
      "265 -0.173734 -0.112681 -0.193751  -0.142566  0.415584 -0.247752 -0.189276   \n",
      "5   -0.054076  0.082191  0.120535   0.302189 -0.584416  0.136863  0.148240   \n",
      "447 -0.071170 -0.116527 -0.199326  -0.360748 -0.584416 -0.016983 -0.180807   \n",
      "245 -0.088264 -0.116527 -0.147061  -0.042706  0.415584 -0.124675 -0.139715   \n",
      "120 -0.088264 -0.052425  0.096841  -0.114594  0.415584  0.198402 -0.099878   \n",
      "41   0.048489 -0.115245 -0.094796  -0.050258  0.415584 -0.355445 -0.071961   \n",
      "322  0.236523  0.075780 -0.119883   0.108903  0.415584 -0.278521 -0.052199   \n",
      "56  -0.173734 -0.116527 -0.121974  -0.258370  0.415584 -0.063137 -0.069451   \n",
      "214 -0.036982 -0.116527 -0.163089  -0.120188  0.415584  0.059940 -0.112739   \n",
      "460 -0.173734  0.056550  0.478723   0.150581  0.415584  0.167632  0.040963   \n",
      "224 -0.054076  0.116806 -0.085040  -0.365783  0.415584  0.106094 -0.205901   \n",
      "377 -0.173734 -0.076463 -0.003507   0.172679 -0.584416 -0.016983  0.035003   \n",
      "419 -0.105358 -0.116527 -0.013263   0.105546  0.415584 -0.078521 -0.034633   \n",
      "177 -0.156640 -0.116527 -0.161695  -0.244664  0.415584 -0.109291 -0.184571   \n",
      "60   0.031395 -0.107553 -0.204901  -0.122426  0.415584  0.059940 -0.075098   \n",
      "44  -0.207922 -0.116527 -0.121974  -0.438230  0.415584  0.013786  0.644162   \n",
      "\n",
      "      alcohol       age  chd  \n",
      "198 -0.017966  0.228245    1  \n",
      "81  -0.072114  0.207836    1  \n",
      "69  -0.115799 -0.220735    1  \n",
      "175 -0.115799  0.371102    1  \n",
      "128  0.084759  0.350694    1  \n",
      "275 -0.096911  0.411918    1  \n",
      "352 -0.036989  0.309877    1  \n",
      "106  0.030951  0.371102    1  \n",
      "113 -0.115799  0.371102    1  \n",
      "191 -0.094805  0.126204    1  \n",
      "228 -0.101803  0.432326    1  \n",
      "83  -0.017966  0.289469    1  \n",
      "135  0.015596  0.187428    1  \n",
      "303 -0.115799  0.330285    1  \n",
      "17  -0.021431  0.207836    1  \n",
      "155  0.699678  0.248653    1  \n",
      "190 -0.080130 -0.057470    1  \n",
      "223 -0.115799  0.044571    1  \n",
      "413 -0.112334  0.411918    1  \n",
      "289  0.072869 -0.098286    1  \n",
      "19  -0.115799  0.330285    1  \n",
      "3    0.049022  0.309877    1  \n",
      "346 -0.115799  0.207836    1  \n",
      "454 -0.059884 -0.077878    1  \n",
      "284  0.334436 -0.037062    1  \n",
      "310 -0.059205  0.330285    1  \n",
      "345  0.420855 -0.343184    1  \n",
      "221 -0.089914  0.064979    1  \n",
      "39   0.040054  0.309877    1  \n",
      "167 -0.028429  0.003755    1  \n",
      "..        ...       ...  ...  \n",
      "136  0.009957 -0.220735    0  \n",
      "174 -0.106491  0.391510    0  \n",
      "290 -0.092020 -0.547266    0  \n",
      "236 -0.094194  0.371102    0  \n",
      "428 -0.115799 -0.179919    0  \n",
      "363  0.251074  0.350694    0  \n",
      "102 -0.089234  0.064979    0  \n",
      "372  0.047052 -0.220735    0  \n",
      "94  -0.114508  0.269061    0  \n",
      "324 -0.017966  0.248653    0  \n",
      "24  -0.115799  0.146612    0  \n",
      "268  0.000650 -0.547266    0  \n",
      "386 -0.115799  0.330285    0  \n",
      "311  0.160239  0.064979    0  \n",
      "265 -0.099357 -0.506449    0  \n",
      "5   -0.019732  0.044571    0  \n",
      "447 -0.115799 -0.526857    0  \n",
      "245 -0.071366  0.289469    0  \n",
      "120 -0.035426  0.350694    0  \n",
      "41  -0.084139 -0.261551    0  \n",
      "322  0.142779  0.371102    0  \n",
      "56  -0.093922 -0.302368    0  \n",
      "214 -0.106491 -0.506449    0  \n",
      "460  0.047052 -0.057470    0  \n",
      "224 -0.101803  0.269061    0  \n",
      "377 -0.087875  0.207836    0  \n",
      "419  0.128783 -0.037062    0  \n",
      "177 -0.115799 -0.526857    0  \n",
      "60  -0.095892 -0.200327    0  \n",
      "44  -0.115799 -0.526857    0  \n",
      "\n",
      "[320 rows x 10 columns]\n",
      "[198, 81, 69, 175, 128, 275, 352, 106, 113, 191, 228, 83, 135, 303, 17, 155, 190, 223, 413, 289, 19, 3, 346, 454, 284, 310, 345, 221, 39, 167, 192, 353, 235, 46, 215, 4, 131, 453, 255, 397, 252, 149, 53, 423, 407, 232, 269, 154, 341, 240, 57, 47, 11, 280, 82, 185, 402, 161, 298, 249, 0, 93, 227, 270, 189, 398, 7, 379, 382, 141, 140, 391, 449, 43, 10, 307, 325, 25, 226, 312, 332, 77, 147, 276, 86, 1, 385, 348, 230, 216, 455, 9, 422, 342, 166, 364, 78, 274, 406, 116, 211, 118, 148, 360, 271, 183, 293, 410, 403, 20, 390, 18, 229, 334, 258, 260, 27, 107, 201, 412, 389, 40, 80, 33, 29, 387, 336, 295, 282, 243, 370, 32, 35, 246, 323, 424, 446, 125, 318, 30, 184, 355, 244, 264, 256, 98, 182, 461, 333, 52, 159, 123, 132, 301, 415, 114, 31, 91, 111, 458]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:13: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n",
      "  del sys.path[0]\n"
     ]
    }
   ],
   "source": [
    "#重组数据集，保证label数量相等\n",
    "one_label_result = all_data[(all_data.chd == 1)]\n",
    "zero_label_result = all_data[(all_data.chd == 0)]\n",
    "\n",
    "one_label_length = len(one_label_result)\n",
    "zero_label_length = len(zero_label_result)\n",
    "\n",
    "small_len = one_label_length if one_label_length < zero_label_length else zero_label_length;\n",
    "\n",
    "one_index = random.sample(list(one_label_result.index.values), small_len)\n",
    "zero_index = random.sample(list(zero_label_result.index.values), small_len)\n",
    "\n",
    "new_data = pd.concat([one_label_result.ix[one_index], zero_label_result.ix[zero_index]])\n",
    "new_data.describe()\n",
    "print(new_data)\n",
    "# print(one_label_length)\n",
    "# print(zero_label_length)\n",
    "# print(small_len)\n",
    "print(one_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          sbp   tobacco       ldl  adiposity   famhist     typea   obesity  \\\n",
      "126  0.082677  0.012960 -0.052287  -0.131937  0.415584  0.106094  0.054451   \n",
      "142 -0.088264 -0.026784  0.055030  -0.310958  0.415584  0.167632 -0.033692   \n",
      "198  0.185241 -0.020373  0.310082   0.029742 -0.584416 -0.216983  0.069193   \n",
      "275 -0.019888  0.165524 -0.003507   0.298273 -0.584416 -0.232368  0.016182   \n",
      "227 -0.036982  0.203986 -0.066225   0.260511  0.415584 -0.170829  0.071703   \n",
      "46   0.202335  0.120652  0.265483  -0.021167 -0.584416  0.167632 -0.010480   \n",
      "376 -0.207922 -0.001143 -0.040441  -0.079069  0.415584  0.106094 -0.048749   \n",
      "114 -0.190828  0.883473 -0.109430  -0.291377  0.415584 -0.093906 -0.208410   \n",
      "424  0.031395 -0.046014 -0.101068  -0.075713  0.415584 -0.140060 -0.074784   \n",
      "370  0.082677  0.146293  0.209733   0.253238 -0.584416 -0.109291  0.015241   \n",
      "290 -0.225016 -0.116527 -0.211172  -0.264244  0.415584 -0.170829 -0.125913   \n",
      "388  0.065583 -0.116527  0.012521  -0.192356  0.415584  0.059940 -0.057845   \n",
      "265 -0.173734 -0.112681 -0.193751  -0.142566  0.415584 -0.247752 -0.189276   \n",
      "56  -0.173734 -0.116527 -0.121974  -0.258370  0.415584 -0.063137 -0.069451   \n",
      "112 -0.036982 -0.052425 -0.075284  -0.299769  0.415584 -0.016983 -0.157281   \n",
      "85  -0.071170 -0.098579 -0.100371   0.152539  0.415584 -0.063137  0.046295   \n",
      "439 -0.071170  0.011678 -0.163089  -0.223405  0.415584  0.106094 -0.125286   \n",
      "23  -0.002794 -0.097296 -0.064831   0.091001  0.415584  0.013786  0.083309   \n",
      "190  0.031395 -0.116527 -0.071103   0.007644  0.415584 -0.078521 -0.052513   \n",
      "294 -0.036982 -0.116527  0.061998   0.103868  0.415584  0.229171  0.197173   \n",
      "120 -0.088264 -0.052425  0.096841  -0.114594  0.415584  0.198402 -0.099878   \n",
      "149 -0.122452  0.380268  0.021580  -0.037671  0.415584 -0.109291 -0.088586   \n",
      "449  0.048489  0.011678  0.020186   0.010441 -0.584416  0.059940  0.047236   \n",
      "57  -0.105358  0.046934 -0.124064   0.030581  0.415584  0.029171 -0.016440   \n",
      "145  0.048489 -0.116527 -0.062740  -0.187041  0.415584  0.044555 -0.123717   \n",
      "11  -0.036982  0.335396 -0.020929  -0.084384 -0.584416  0.183017 -0.092664   \n",
      "174 -0.088264 -0.116527 -0.155423   0.110301 -0.584416 -0.001598 -0.125286   \n",
      "10   0.578403  0.075780 -0.124761   0.191980  0.415584  0.290709  0.024024   \n",
      "42  -0.156640 -0.116527 -0.255772  -0.262566  0.415584 -0.093906 -0.122149   \n",
      "365 -0.054076 -0.093450 -0.025807  -0.164104  0.415584 -0.078521  0.002067   \n",
      "..        ...       ...       ...        ...       ...       ...       ...   \n",
      "282  0.578403 -0.116527 -0.039744   0.218833  0.415584  0.244555  0.041276   \n",
      "69  -0.139546 -0.116527 -0.095493  -0.260328  0.415584  0.213786 -0.156340   \n",
      "160 -0.105358 -0.058835  0.103113  -0.159349  0.415584  0.183017 -0.038711   \n",
      "127 -0.019888  0.010396 -0.138002   0.136315 -0.584416 -0.047752  0.262732   \n",
      "213 -0.054076 -0.116527 -0.262044  -0.112356  0.415584  0.136863  0.022142   \n",
      "12  -0.173734 -0.116527 -0.199326  -0.429559  0.415584  0.090709 -0.140342   \n",
      "363  0.236523 -0.097296 -0.161695   0.241210 -0.584416 -0.001598  0.028729   \n",
      "81   0.014301  0.144370  0.013218   0.477854  0.415584 -0.001598  0.617186   \n",
      "47  -0.190828 -0.055309  0.196493   0.029182 -0.584416 -0.016983  0.124400   \n",
      "361 -0.122452 -0.103707 -0.074587   0.009882  0.415584 -0.155445  0.063861   \n",
      "179 -0.259204  0.364242  0.011824   0.258553  0.415584 -0.186214  0.060097   \n",
      "419 -0.105358 -0.116527 -0.013263   0.105546  0.415584 -0.078521 -0.034633   \n",
      "427  0.065583 -0.079348 -0.171451   0.255196  0.415584 -0.047752  0.083623   \n",
      "314  0.441651  0.048537  0.089873   0.312819  0.415584 -0.170829  0.133811   \n",
      "167  0.185241 -0.067809  0.235517   0.108903 -0.584416  0.013786 -0.005461   \n",
      "308  0.031395 -0.039604 -0.152636  -0.042426  0.415584  0.013786  0.001439   \n",
      "385  0.185241  0.018088  0.140744   0.351980 -0.584416  0.121479  0.215367   \n",
      "288 -0.130999 -0.114925 -0.009082  -0.327741  0.415584 -0.032368 -0.088272   \n",
      "180  0.236523 -0.116527 -0.029988   0.247924  0.415584 -0.124675  0.128478   \n",
      "82   0.082677  0.037319  0.094054   0.311700 -0.584416  0.152248 -0.018950   \n",
      "202 -0.036982 -0.028386  0.053636   0.021350  0.415584  0.059940  0.120009   \n",
      "324 -0.019888 -0.062040 -0.084343  -0.147601  0.415584  0.044555 -0.207155   \n",
      "246 -0.019888 -0.103707 -0.057862  -0.120468 -0.584416  0.152248 -0.117444   \n",
      "456  0.270711 -0.103707 -0.043925   0.465826 -0.584416  0.044555  0.221326   \n",
      "226 -0.225016  0.026421  0.170012   0.023588 -0.584416  0.244555  0.039081   \n",
      "430 -0.173734 -0.116527 -0.059256  -0.264244  0.415584  0.183017 -0.183943   \n",
      "305 -0.088264 -0.116527 -0.105946   0.031980 -0.584416 -0.216983  0.017123   \n",
      "77   0.014301 -0.097296  0.057120   0.223308 -0.584416  0.075325  0.035944   \n",
      "220 -0.002794 -0.116527 -0.111521  -0.375013  0.415584  0.013786 -0.180807   \n",
      "341 -0.190828  0.020652  0.158862  -0.151517 -0.584416  0.229171 -0.085763   \n",
      "\n",
      "      alcohol       age  \n",
      "126 -0.103909 -0.302368  \n",
      "142 -0.112334 -0.098286  \n",
      "198 -0.017966  0.228245  \n",
      "275 -0.096911  0.411918  \n",
      "227  0.079867  0.187428  \n",
      "46  -0.075986  0.309877  \n",
      "376  0.327914 -0.241143  \n",
      "114  0.217512  0.330285  \n",
      "424 -0.077345 -0.016653  \n",
      "370 -0.074763  0.432326  \n",
      "290 -0.092020 -0.547266  \n",
      "388  0.121785 -0.343184  \n",
      "265 -0.099357 -0.506449  \n",
      "56  -0.093922 -0.302368  \n",
      "112 -0.101803 -0.118694  \n",
      "85   0.110643  0.044571  \n",
      "439 -0.115799 -0.057470  \n",
      "23  -0.105879  0.309877  \n",
      "190 -0.080130 -0.057470  \n",
      "294 -0.102075 -0.179919  \n",
      "120 -0.035426  0.350694  \n",
      "149 -0.115799  0.371102  \n",
      "449  0.495656  0.105796  \n",
      "57  -0.031961 -0.098286  \n",
      "145 -0.083188 -0.057470  \n",
      "11  -0.115799 -0.057470  \n",
      "174 -0.106491  0.391510  \n",
      "10   0.265070  0.350694  \n",
      "42  -0.115799 -0.567674  \n",
      "365  0.219618 -0.302368  \n",
      "..        ...       ...  \n",
      "282 -0.073880  0.146612  \n",
      "69  -0.115799 -0.220735  \n",
      "160 -0.111111 -0.241143  \n",
      "127  0.009957 -0.098286  \n",
      "213 -0.115799  0.207836  \n",
      "12  -0.115799 -0.526857  \n",
      "363  0.251074  0.350694  \n",
      "81  -0.072114  0.207836  \n",
      "47  -0.091340 -0.200327  \n",
      "361  0.023953 -0.179919  \n",
      "179 -0.017966  0.269061  \n",
      "419  0.128783 -0.037062  \n",
      "427  0.189929  0.126204  \n",
      "314  0.373365  0.146612  \n",
      "167 -0.028429  0.003755  \n",
      "308  0.285995 -0.118694  \n",
      "385 -0.094805  0.228245  \n",
      "288 -0.096911 -0.547266  \n",
      "180 -0.025643  0.269061  \n",
      "82  -0.109820  0.248653  \n",
      "202 -0.059205 -0.200327  \n",
      "324 -0.017966  0.248653  \n",
      "246 -0.115799  0.269061  \n",
      "456 -0.101803  0.289469  \n",
      "226 -0.115799 -0.220735  \n",
      "430 -0.115799 -0.547266  \n",
      "305 -0.002272  0.126204  \n",
      "77  -0.115799  0.248653  \n",
      "220 -0.115799 -0.547266  \n",
      "341 -0.115799  0.187428  \n",
      "\n",
      "[288 rows x 9 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:6: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# 数据分10份，拿一份做测试集，九份做训练集\n",
    "train_data_size = int(small_len * 2 * 0.9)\n",
    "test_data_size = int(small_len * 2 * 0.1)\n",
    "\n",
    "train_data_index = random.sample(list(new_data.index.values), train_data_size)\n",
    "train_data = new_data.ix[train_data_index]\n",
    "\n",
    "test_data_index = list(set(new_data.index.values).difference(set(train_data_index)))\n",
    "test_data = new_data.ix[test_data_index]\n",
    "\n",
    "train_label = train_data['chd']\n",
    "train_data = train_data.loc[:,raw_data.columns[:9]]\n",
    "\n",
    "test_label = test_data['chd']\n",
    "test_data = test_data.loc[:,raw_data.columns[:9]]\n",
    "# print(train_data.shape, train_label.shape)\n",
    "# print(test_data.shape, test_label.shape)\n",
    "print(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paramaters for the model\n",
    "# learning_rate = 0.01\n",
    "# batch_size = 16\n",
    "# n_epochs = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = tf.placeholder(dtype = np.float32, shape = [batch_size, 9], name='X')\n",
    "# Y = tf.placeholder(dtype = np.float32, shape = [batch_size, 2], name='Y')\n",
    "\n",
    "# W1 = tf.Variable(tf.random_normal([9, 20]), name='W1')\n",
    "# b1 = tf.Variable(tf.random_normal([1, 20]), name='b1')\n",
    "\n",
    "# W2 = tf.Variable(tf.random_normal([20, 10]), name='W2')\n",
    "# b2 = tf.Variable(tf.random_normal([1, 10]), name='b2')\n",
    "\n",
    "# W3 = tf.Variable(tf.random_normal([10, 2]), name='W3')\n",
    "# b3 = tf.Variable(tf.random_normal([1, 2]), name='b3')\n",
    "\n",
    "# Z1 = tf.matmul(X, W1) + b1\n",
    "# A1 = tf.nn.relu(Z1)\n",
    "# Z2 = tf.matmul(A1, W2) + b2\n",
    "# A2 = tf.nn.relu(Z2)\n",
    "# logits = tf.matmul(A2, W3) + b3\n",
    "\n",
    "# entropy = tf.nn.softmax_cross_entropy_with_logits(labels = Y, logits = logits)\n",
    "\n",
    "# loss = tf.reduce_mean(entropy)\n",
    "\n",
    "# optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "\n",
    "# def to_one_hotting(labels):\n",
    "#     return (np.arange(2) == labels[:,None]).astype(np.float32)\n",
    "\n",
    "# with tf.Session() as sess:\n",
    "#     start_time = time.time()\n",
    "#     sess.run(tf.global_variables_initializer())\t\n",
    "#     n_batches = int(len(train_data)/batch_size)\n",
    "#     for i in range(n_epochs): \n",
    "#         total_loss = 0\n",
    "\n",
    "#         for index in range(n_batches):\n",
    "#             X_batch = train_data[index*batch_size:(index+1)*batch_size].values\n",
    "#             Y_batch = train_label[index*batch_size:(index+1)*batch_size].values\n",
    "#             Y_batch = to_one_hotting(Y_batch)\n",
    "#             _, loss_batch, get_entropy, get_logits = sess.run([optimizer, loss, entropy, logits], feed_dict={X: X_batch, Y: Y_batch})\n",
    "#             total_loss += loss_batch\n",
    "            \n",
    "#         if i%100 == 0:\n",
    "#             print('Average loss epoch :{0}'.format(total_loss/n_batches))\n",
    "\n",
    "#     print('Total time: {0} seconds'.format(time.time() - start_time))\n",
    "\n",
    "#     print('Optimization Finished!')\n",
    "\n",
    "#     # test the model\n",
    "#     n_batches = int(len(test_data)/batch_size)\n",
    "#     total_correct_preds = 0\n",
    "#     for index in range(n_batches):\n",
    "#         X_batch = test_data[index*batch_size:(index+1)*batch_size].values\n",
    "#         Y_batch = test_label[index*batch_size:(index+1)*batch_size].values\n",
    "#         Y_batch = to_one_hotting(Y_batch)\n",
    "#         _, loss_batch, logits_batch = sess.run([optimizer, loss, logits], feed_dict={X: X_batch, Y:Y_batch}) \n",
    "#         preds = tf.nn.softmax(logits_batch)\n",
    "#         correct_preds = tf.equal(tf.argmax(preds, 1), tf.argmax(Y_batch, 1))\n",
    "#         accuracy = tf.reduce_sum(tf.cast(correct_preds, tf.float32)) # need numpy.count_nonzero(boolarr) :(\n",
    "#         total_correct_preds += sess.run(accuracy)\n",
    "\n",
    "#     print('Accuracy:',format(total_correct_preds/len(test_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: create_placeholders\n",
    "\n",
    "def create_placeholders(n_x, n_y):\n",
    "    \"\"\"\n",
    "    Creates the placeholders for the tensorflow session.\n",
    "    \n",
    "    Arguments:\n",
    "    n_x -- scalar, size of an image vector (num_px * num_px = 64 * 64 * 3 = 12288)\n",
    "    n_y -- scalar, number of classes (from 0 to 5, so -> 6)\n",
    "    \n",
    "    Returns:\n",
    "    X -- placeholder for the data input, of shape [n_x, None] and dtype \"float\"\n",
    "    Y -- placeholder for the input labels, of shape [n_y, None] and dtype \"float\"\n",
    "    \n",
    "    Tips:\n",
    "    - You will use None because it let's us be flexible on the number of examples you will for the placeholders.\n",
    "      In fact, the number of examples during test/train is different.\n",
    "    \"\"\"\n",
    "\n",
    "    ### START CODE HERE ### (approx. 2 lines)\n",
    "    X = tf.placeholder(shape=[n_x, None], dtype=tf.float32)\n",
    "    Y = tf.placeholder(shape=[n_y, None], dtype=tf.float32)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: initialize_parameters\n",
    "\n",
    "def initialize_parameters():\n",
    "    \"\"\"\n",
    "    Initializes parameters to build a neural network with tensorflow. The shapes are:\n",
    "                        W1 : [9, 20]\n",
    "                        b1 : [1, 20]\n",
    "                        W2 : [20, 10]\n",
    "                        b2 : [1, 10]\n",
    "                        W3 : [10, 2]\n",
    "                        b3 : [1 2]\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- a dictionary of tensors containing W1, b1, W2, b2, W3, b3\n",
    "    \"\"\"\n",
    "    \n",
    "    tf.set_random_seed(1)                   # so that your \"random\" numbers match ours\n",
    "        \n",
    "    ### START CODE HERE ### (approx. 6 lines of code)\n",
    "#     W1 = tf.get_variable(\"W1\", [20, 288], initializer = tf.contrib.layers.xavier_initializer())\n",
    "#     b1 = tf.get_variable(\"b1\", [20,1], initializer = tf.contrib.layers.xavier_initializer())\n",
    "#     W2 = tf.get_variable(\"W2\", [10,20], initializer = tf.contrib.layers.xavier_initializer())\n",
    "#     b2 = tf.get_variable(\"b2\", [10,1], initializer = tf.contrib.layers.xavier_initializer())\n",
    "#     W3 = tf.get_variable(\"W3\", [2,10], initializer = tf.contrib.layers.xavier_initializer())\n",
    "#     b3 = tf.get_variable(\"b3\", [2,1], initializer = tf.contrib.layers.xavier_initializer())\n",
    "    \n",
    "    W1 = tf.Variable(tf.random_normal([25, 288]), name='W1')\n",
    "    b1 = tf.Variable(tf.random_normal([25, 1]), name='b1')\n",
    "\n",
    "    W2 = tf.Variable(tf.random_normal([12, 25]), name='W2')\n",
    "    b2 = tf.Variable(tf.random_normal([12, 1]), name='b2')\n",
    "\n",
    "    W3 = tf.Variable(tf.random_normal([6, 12]), name='W3')\n",
    "    b3 = tf.Variable(tf.random_normal([6, 1]), name='b3')\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2,\n",
    "                  \"W3\": W3,\n",
    "                  \"b3\": b3}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: forward_propagation\n",
    "\n",
    "def forward_propagation(X, parameters):\n",
    "    \"\"\"\n",
    "    Implements the forward propagation for the model: LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SOFTMAX\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input dataset placeholder, of shape (input size, number of examples)\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", \"W2\", \"b2\", \"W3\", \"b3\"\n",
    "                  the shapes are given in initialize_parameters\n",
    "\n",
    "    Returns:\n",
    "    Z3 -- the output of the last LINEAR unit\n",
    "    \"\"\"\n",
    "    \n",
    "    # Retrieve the parameters from the dictionary \"parameters\" \n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']\n",
    "    W3 = parameters['W3']\n",
    "    b3 = parameters['b3']\n",
    "    \n",
    "    ### START CODE HERE ### (approx. 5 lines)              # Numpy Equivalents:\n",
    "    Z1 = tf.add(tf.matmul(W1, X), b1)                                            # Z1 = np.dot(W1, X) + b1\n",
    "    A1 = tf.nn.relu(Z1)                                              # A1 = relu(Z1)\n",
    "    Z2 = tf.add(tf.matmul(W2, A1), b2)                                              # Z2 = np.dot(W2, a1) + b2\n",
    "    A2 = tf.nn.relu(Z2)                                              # A2 = relu(Z2)\n",
    "    Z3 = tf.add(tf.matmul(W3, A2), b3)                                              # Z3 = np.dot(W3,Z2) + b3\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return Z3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: compute_cost \n",
    "\n",
    "def compute_cost(Z3, Y):\n",
    "    \"\"\"\n",
    "    Computes the cost\n",
    "    \n",
    "    Arguments:\n",
    "    Z3 -- output of forward propagation (output of the last LINEAR unit), of shape (6, number of examples)\n",
    "    Y -- \"true\" labels vector placeholder, same shape as Z3\n",
    "    \n",
    "    Returns:\n",
    "    cost - Tensor of the cost function\n",
    "    \"\"\"\n",
    "    \n",
    "    # to fit the tensorflow requirement for tf.nn.softmax_cross_entropy_with_logits(...,...)\n",
    "    logits = tf.transpose(Z3)\n",
    "    labels = tf.transpose(Y)\n",
    "    \n",
    "    ### START CODE HERE ### (1 line of code)\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits = logits, labels = labels))\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X_train, Y_train, X_test, Y_test, learning_rate = 0.0001,\n",
    "          num_epochs = 1500, minibatch_size = 32, print_cost = True):\n",
    "    \"\"\"\n",
    "    Implements a three-layer tensorflow neural network: LINEAR->RELU->LINEAR->RELU->LINEAR->SOFTMAX.\n",
    "    \n",
    "    Arguments:\n",
    "    X_train -- training set, of shape (input size = 12288, number of training examples = 1080)\n",
    "    Y_train -- test set, of shape (output size = 6, number of training examples = 1080)\n",
    "    X_test -- training set, of shape (input size = 12288, number of training examples = 120)\n",
    "    Y_test -- test set, of shape (output size = 6, number of test examples = 120)\n",
    "    learning_rate -- learning rate of the optimization\n",
    "    num_epochs -- number of epochs of the optimization loop\n",
    "    minibatch_size -- size of a minibatch\n",
    "    print_cost -- True to print the cost every 100 epochs\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "    \n",
    "#     ops.reset_default_graph()                         # to be able to rerun the model without overwriting tf variables\n",
    "    (n_x, m) = X_train.shape                          # (n_x: input size, m : number of examples in the train set)\n",
    "    n_y = Y_train.shape[0]                            # n_y : output size\n",
    "    costs = []                                        # To keep track of the cost\n",
    "    \n",
    "    # Create Placeholders of shape (n_x, n_y)\n",
    "    ### START CODE HERE ### (1 line)\n",
    "    X, Y = create_placeholders(n_x, n_y)\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    # Initialize parameters\n",
    "    ### START CODE HERE ### (1 line)\n",
    "    parameters = initialize_parameters()\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Forward propagation: Build the forward propagation in the tensorflow graph\n",
    "    ### START CODE HERE ### (1 line)\n",
    "    Z3 = forward_propagation(X, parameters)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Cost function: Add cost function to tensorflow graph\n",
    "    ### START CODE HERE ### (1 line)\n",
    "    cost = compute_cost(Z3, Y)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Backpropagation: Define the tensorflow optimizer. Use an AdamOptimizer.\n",
    "    ### START CODE HERE ### (1 line)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Initialize all the variables\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    # Start the session to compute the tensorflow graph\n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        # Run the initialization\n",
    "        sess.run(init)\n",
    "        \n",
    "        # Do the training loop\n",
    "        for epoch in range(num_epochs):\n",
    "\n",
    "            epoch_cost = 0.                       # Defines a cost related to an epoch\n",
    "            num_minibatches = int(m / minibatch_size) # number of minibatches of size minibatch_size in the train set\n",
    "            minibatches = random_mini_batches(X_train, Y_train, minibatch_size)\n",
    "\n",
    "            for minibatch in minibatches:\n",
    "\n",
    "                # Select a minibatch\n",
    "                (minibatch_X, minibatch_Y) = minibatch\n",
    "                \n",
    "                # IMPORTANT: The line that runs the graph on a minibatch.\n",
    "                # Run the session to execute the \"optimizer\" and the \"cost\", the feedict should contain a minibatch for (X,Y).\n",
    "                ### START CODE HERE ### (1 line)\n",
    "                _ , minibatch_cost = sess.run([optimizer, cost], feed_dict={X: minibatch_X, Y: minibatch_Y})\n",
    "                ### END CODE HERE ###\n",
    "                \n",
    "                epoch_cost += minibatch_cost / num_minibatches\n",
    "\n",
    "            # Print the cost every epoch\n",
    "            if print_cost == True and epoch % 100 == 0:\n",
    "                print (\"Cost after epoch %i: %f\" % (epoch, epoch_cost))\n",
    "            if print_cost == True and epoch % 5 == 0:\n",
    "                costs.append(epoch_cost)\n",
    "                \n",
    "        # plot the cost\n",
    "        plt.plot(np.squeeze(costs))\n",
    "        plt.ylabel('cost')\n",
    "        plt.xlabel('iterations (per tens)')\n",
    "        plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "        plt.show()\n",
    "\n",
    "        # lets save the parameters in a variable\n",
    "        parameters = sess.run(parameters)\n",
    "        print (\"Parameters have been trained!\")\n",
    "\n",
    "        # Calculate the correct predictions\n",
    "        correct_prediction = tf.equal(tf.argmax(Z3), tf.argmax(Y))\n",
    "\n",
    "        # Calculate accuracy on the test set\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "\n",
    "        print (\"Train Accuracy:\", accuracy.eval({X: X_train, Y: Y_train}))\n",
    "        print (\"Test Accuracy:\", accuracy.eval({X: X_test, Y: Y_test}))\n",
    "        \n",
    "        return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-11-412488f44149>:20: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'slice'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-e06ad43e2047>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mparameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_label\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-9bf3b47b85c6>\u001b[0m in \u001b[0;36mmodel\u001b[0;34m(X_train, Y_train, X_test, Y_test, learning_rate, num_epochs, minibatch_size, print_cost)\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0mepoch_cost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.\u001b[0m                       \u001b[0;31m# Defines a cost related to an epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0mnum_minibatches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mminibatch_size\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# number of minibatches of size minibatch_size in the train set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m             \u001b[0mminibatches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom_mini_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminibatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mminibatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mminibatches\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/notebooks/predict_disease/tf_utils.py\u001b[0m in \u001b[0;36mrandom_mini_batches\u001b[0;34m(X, Y, mini_batch_size, seed)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;31m# Step 1: Shuffle (X, Y)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mpermutation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermutation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0mshuffled_X\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpermutation\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m     \u001b[0mshuffled_Y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpermutation\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2137\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2138\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2139\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2141\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_getitem_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_getitem_column\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2144\u001b[0m         \u001b[0;31m# get column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2145\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2146\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_item_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2148\u001b[0m         \u001b[0;31m# duplicate columns & possible reduce dimensionality\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_get_item_cache\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m   1838\u001b[0m         \u001b[0;34m\"\"\"Return the cached item, item represents a label indexer.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1839\u001b[0m         \u001b[0mcache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_item_cache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1840\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1841\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1842\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'slice'"
     ]
    }
   ],
   "source": [
    "parameters = model(train_data, train_label, test_data, test_label)\n",
    "print(train_data.shape)\n",
    "print(train_label.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
